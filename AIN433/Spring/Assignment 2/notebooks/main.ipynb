{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AIN433 Assignment 2 - Image classification using Bag of Visual Words\n",
    "\n",
    "Ahmet Emre Usta\n",
    "\n",
    "2200765036\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This notebook addresses the task of image classification using the Bag of Visual Words (BOVW) model, leveraging keypoint description methods such as SIFT/SURF and ORB, combined with KMeans clustering. The BOVW model represents images as a collection of distinct features - keypoints and descriptors, facilitating image classification and similarity identification. The assignment explores the process of extracting these features, matching them across images, and classifying the images based on the generated features. This approach is central to many computer vision tasks and is foundational for understanding more complex models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Visual Words (BOVW) Model\n",
    "\n",
    "The Bag of Visual Words model is an approach used in computer vision for image classification and retrieval. It involves representing images through the aggregation of local features. Key points of an image are identified, descriptors for these keypoints are generated, and a visual dictionary (or vocabulary) is created by clustering these descriptors. Each image is then represented as a frequency histogram of these features, allowing for efficient comparison and classification.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective\n",
    "\n",
    "The objective of this assignment is to implement the BOVW framework from scratch, focusing on the following steps:\n",
    "\n",
    "- **Keypoint Detection:** Using SIFT or Harris-Laplacian for identifying distinct points in an image.\n",
    "- **Feature Extraction:** Extracting keypoints using methods like SIFT/SURF and ORB.\n",
    "- **Feature Matching:** Matching features across images based on Euclidean distance.\n",
    "- **BoW Formation:** Clustering features to form a visual dictionary and quantizing images to histograms.\n",
    "- **Classification:** Employing the k-NN approach to classify images and evaluating the performance of different visual vocabularies.\n",
    "\n",
    "Through these steps, we aim to explore the effectiveness of different keypoint description methods and clustering approaches in classifying images and understanding the impact of various factors on the accuracy and runtime of the classification.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importance of SIFT/SURF, ORB, and KMeans Clustering\n",
    "\n",
    "- **SIFT/SURF:** These are feature detection algorithms that identify and describe local features in images. They are robust to changes in scale, rotation, and illumination. SIFT and SURF differ in their complexity and speed, providing a trade-off between accuracy and computational efficiency.\n",
    "- **ORB:** A fast feature detector and descriptor, ORB is designed to achieve similar performance to SIFT but at a lower computational cost. It is particularly useful for real-time applications.\n",
    "- **KMeans Clustering:** This clustering method is used to group the extracted features into a set number of clusters, forming the visual vocabulary. KMeans is chosen for its simplicity and efficiency in creating a compact visual dictionary that can represent a wide range of images.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import cv2\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "import tarfile\n",
    "import os\n",
    "from sklearn.cluster import KMeans\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set working directory\n",
    "workdir = \"/Users/emre/GitHub/HU-AI/AIN433/Spring/Assignment 2/\"\n",
    "DATASET_PATH = os.path.join(workdir, \"dataset\")\n",
    "SAMPLE_IMAGE_NUMBER = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define URL, target path, and expected directory or file name within the dataset\n",
    "url = \"https://s3.amazonaws.com/fast-ai-imageclas/imagenette2-160.tgz\"\n",
    "imagenette_dir = os.path.basename(url).replace(\".tgz\", \"\")\n",
    "\n",
    "\n",
    "# Function to check if the dataset is already extracted\n",
    "def is_dataset_extracted(dataset_path, expected_entity):\n",
    "    \"\"\"Check if the expected directory or file from the dataset already exists.\"\"\"\n",
    "    return os.path.exists(os.path.join(dataset_path, expected_entity))\n",
    "\n",
    "\n",
    "# Proceed only if the dataset isn't already extracted\n",
    "if not is_dataset_extracted(DATASET_PATH, imagenette_dir):\n",
    "    # Ensure the directory for the dataset exists\n",
    "    os.makedirs(DATASET_PATH, exist_ok=True)\n",
    "\n",
    "    # Perform the request and check if the response is OK (200)\n",
    "    with requests.get(url, stream=True) as response:\n",
    "        response.raise_for_status()  # Raises an HTTPError for bad responses\n",
    "        content_type = response.headers.get(\"Content-Type\")\n",
    "        expected_types = [\"application/octet-stream\", \"application/x-tar\"]\n",
    "        if content_type not in expected_types:\n",
    "            raise ValueError(\n",
    "                f\"Unexpected Content-Type: {content_type}. Expected one of: {expected_types}.\"\n",
    "            )\n",
    "\n",
    "        total_size_in_bytes = int(response.headers.get(\"content-length\", 0))\n",
    "        progress_bar = tqdm(total=total_size_in_bytes, unit=\"iB\", unit_scale=True)\n",
    "\n",
    "        # Direct extraction from the response stream\n",
    "        with tarfile.open(fileobj=response.raw, mode=\"r|gz\") as tar:\n",
    "            tar.extractall(DATASET_PATH)\n",
    "\n",
    "        progress_bar.close()\n",
    "\n",
    "    # Success message\n",
    "    print(f\"Download and extraction of {url} completed successfully!\")\n",
    "else:\n",
    "    print(\"Dataset already exists. Skipping download and extraction.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imagenette_paths = pd.read_csv(\n",
    "    os.path.join(DATASET_PATH, \"imagenette2-160\", \"noisy_imagenette.csv\")\n",
    ")\n",
    "df_imagenette_paths.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imagenette_paths.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample the first 5 images for every label group\n",
    "sample = df_imagenette_paths.groupby(\"noisy_labels_0\").head(SAMPLE_IMAGE_NUMBER)\n",
    "num_labels = df_imagenette_paths[\"noisy_labels_0\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create subplots\n",
    "fig, axs = plt.subplots(\n",
    "    num_labels, SAMPLE_IMAGE_NUMBER, figsize=(20, 2 * num_labels), squeeze=False\n",
    ")\n",
    "fig.suptitle(\"Sample Images from Each Label\", fontsize=22, y=1.03)\n",
    "\n",
    "# Create a mapping from labels to subplot row indices\n",
    "label_to_index = {\n",
    "    label: idx\n",
    "    for idx, label in enumerate(sorted(df_imagenette_paths[\"noisy_labels_0\"].unique()))\n",
    "}\n",
    "\n",
    "# Loop through the dataframe and plot\n",
    "for label, group in sample.groupby(\"noisy_labels_0\"):\n",
    "    for i, (_, row) in enumerate(group.iterrows()):\n",
    "        img_path = os.path.join(DATASET_PATH, imagenette_dir, row[\"path\"])\n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        row_index = label_to_index[label]\n",
    "        ax = axs[row_index, i]  # Adjust subplot access\n",
    "        ax.imshow(img)\n",
    "        if i == 0:\n",
    "            ax.set_ylabel(label, rotation=30, size=\"large\", labelpad=60)\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keypoint Detection\n",
    "\n",
    "This section covers the detection of keypoints in images, which are distinctive points that can be reliably detected and described. Keypoint detection is crucial for understanding the structure and features of images, forming the foundation for further processing such as feature extraction and matching.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Keypoint Detection Function\n",
    "\n",
    "We define a function to perform keypoint detection using the SIFT algorithm. The function will take an image as input and return the image with keypoints drawn on it, along with the keypoints themselves.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_keypoints(image_path, method=\"SIFT\"):\n",
    "    # Load the image\n",
    "    image = cv2.imread(image_path)\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Initialize the keypoint detector\n",
    "    if method == \"SIFT\":\n",
    "        detector = cv2.SIFT_create()\n",
    "    elif method == \"SURF\":\n",
    "        detector = cv2.xfeatures2d.SURF_create()\n",
    "    elif method == \"ORB\":\n",
    "        detector = cv2.ORB_create()\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported method: {}\".format(method))\n",
    "\n",
    "    # Detect keypoints\n",
    "    keypoints, _ = detector.detectAndCompute(gray, None)\n",
    "\n",
    "    # Draw keypoints on the image\n",
    "    image_with_keypoints = cv2.drawKeypoints(\n",
    "        image, keypoints, None, flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS\n",
    "    )\n",
    "\n",
    "    return image_with_keypoints, keypoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display Keypoints on Sample Image\n",
    "\n",
    "Let's apply the keypoint detection function to a sample image and visualize the keypoints.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to a sample image\n",
    "group_name = sample.iloc[0][\"noisy_labels_0\"]\n",
    "sample_image_path = os.path.join(DATASET_PATH, imagenette_dir, sample.iloc[0][\"path\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_with_keypoints, keypoints = detect_keypoints(sample_image_path, method=\"SIFT\")\n",
    "\n",
    "# Display the image with keypoints\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.imshow(cv2.cvtColor(image_with_keypoints, cv2.COLOR_BGR2RGB))\n",
    "plt.title(f\"Group: {group_name}\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction\n",
    "\n",
    "In this section, we'll focus on extracting features from images using different methods. Feature extraction is a crucial step in image processing and computer vision applications, allowing us to reduce the amount of resources required to describe a large set of data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Feature Extraction Functions\n",
    "\n",
    "We define functions to perform feature extraction using SIFT, SURF, and ORB. These functions will take an image as input and return the keypoints and descriptors extracted from the image.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_sift(image_path):\n",
    "    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    sift = cv2.SIFT_create()\n",
    "    keypoints, descriptors = sift.detectAndCompute(image, None)\n",
    "    return keypoints, descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_surf(image_path):\n",
    "    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    surf = cv2.xfeatures2d.SURF_create()\n",
    "    keypoints, descriptors = surf.detectAndCompute(image, None)\n",
    "    return keypoints, descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_orb(image_path):\n",
    "    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    orb = cv2.ORB_create()\n",
    "    keypoints, descriptors = orb.detectAndCompute(image, None)\n",
    "    return keypoints, descriptors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization of Extracted Features\n",
    "\n",
    "For illustrative purposes, we will display the keypoints detected by each method on a sample image.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_keypoints(image_path, method_function, title):\n",
    "    \"\"\"\n",
    "    Displays an image with keypoints overlay.\n",
    "\n",
    "    - image_path: Path to the image file.\n",
    "    - method_function: Function to use for keypoint detection.\n",
    "                            It should return a tuple of keypoints and descriptors.\n",
    "    - title: Title to display on the image plot.\n",
    "    \"\"\"\n",
    "    keypoints, descriptors = method_function(image_path)\n",
    "    image = cv2.imread(image_path)\n",
    "    image_keypoints = cv2.drawKeypoints(\n",
    "        image, keypoints, None, flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS\n",
    "    )\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.imshow(cv2.cvtColor(image_keypoints, cv2.COLOR_BGR2RGB))\n",
    "    plt.title(title)  # Set the passed title for the plot\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the group name for the sample image\n",
    "group_name = sample.iloc[0][\"noisy_labels_0\"]\n",
    "\n",
    "# Display keypoints for the sample image using SIFT, with group name as the title\n",
    "display_keypoints(sample_image_path, extract_features_sift, f\"Group: {group_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Matching\n",
    "\n",
    "Feature matching involves comparing the descriptors of two sets of features (from two images) to find matches between them. This step is critical for tasks such as image recognition and alignment. We'll implement feature matching using Euclidean distance as a metric.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Feature Matching Function\n",
    "\n",
    "This function will perform feature matching between two sets of descriptors, using the Euclidean distance for finding the best matches. It will return the matches found.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_features(descriptors1, descriptors2, method=\"BF\"):\n",
    "    # Initialize the matcher\n",
    "    if method == \"BF\":\n",
    "        # Brute Force Matcher with default norms depending on the descriptor type\n",
    "        matcher = cv2.BFMatcher()\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported method: {method}\")\n",
    "\n",
    "    # Match descriptors\n",
    "    matches = matcher.knnMatch(descriptors1, descriptors2, k=2)\n",
    "\n",
    "    # Apply ratio test\n",
    "    good_matches = []\n",
    "    for m, n in matches:\n",
    "        if m.distance < 0.75 * n.distance:\n",
    "            good_matches.append(m)\n",
    "\n",
    "    return good_matches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform Feature Matching on Sample Images\n",
    "\n",
    "We will apply the feature matching function to descriptors extracted from two sample images and visualize the best matches.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_matched_features(\n",
    "    image_path1,\n",
    "    image_path2,\n",
    "    feature_extractor,\n",
    "    feature_matcher,\n",
    "    title=\"Matched Features\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Extracts features from two images, matches them, and displays the matched features with an optional title.\n",
    "\n",
    "    - image_path1: Path to the first image.\n",
    "    - image_path2: Path to the second image.\n",
    "    - feature_extractor: Function to extract features. Should return keypoints and descriptors.\n",
    "    - feature_matcher: Function to match features. Takes two sets of descriptors as input.\n",
    "    - title: Optional title for the plot.\n",
    "    \"\"\"\n",
    "    # Extract features from both images\n",
    "    keypoints1, descriptors1 = feature_extractor(image_path1)\n",
    "    keypoints2, descriptors2 = feature_extractor(image_path2)\n",
    "\n",
    "    # Perform feature matching\n",
    "    matches = feature_matcher(descriptors1, descriptors2)\n",
    "\n",
    "    # Load images and create a matched image\n",
    "    img1 = cv2.imread(image_path1)\n",
    "    img2 = cv2.imread(image_path2)\n",
    "    matched_image = cv2.drawMatches(\n",
    "        img1,\n",
    "        keypoints1,\n",
    "        img2,\n",
    "        keypoints2,\n",
    "        matches,\n",
    "        None,\n",
    "        flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS,\n",
    "    )\n",
    "\n",
    "    # Convert BGR to RGB for displaying\n",
    "    matched_image_rgb = cv2.cvtColor(matched_image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Display the matched features with title\n",
    "    plt.figure(figsize=(16, 8))\n",
    "    plt.imshow(matched_image_rgb)\n",
    "    plt.title(title, fontsize=22)  # Set the plot title\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "sample_image_path1 = os.path.join(DATASET_PATH, imagenette_dir, sample.iloc[0][\"path\"])\n",
    "sample_image_path2 = os.path.join(DATASET_PATH, imagenette_dir, sample.iloc[1][\"path\"])\n",
    "\n",
    "group_name1 = sample.iloc[0][\"noisy_labels_0\"]\n",
    "group_name2 = sample.iloc[1][\"noisy_labels_0\"]\n",
    "\n",
    "# Call the function with a specific title\n",
    "display_matched_features(\n",
    "    sample_image_path1,\n",
    "    sample_image_path2,\n",
    "    extract_features_sift,\n",
    "    match_features,\n",
    "    title=f\"Matched Features: {group_name1} vs. {group_name2}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BoW Formation\n",
    "\n",
    "The Bag of Words (BoW) model in computer vision is a simplification where images are represented as bags of individual features. This part of the notebook covers clustering the features extracted from images to form a BoW dictionary and quantizing images based on this dictionary to create feature histograms.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Functions for BoW Formation\n",
    "\n",
    "We define functions for clustering features to create the BoW dictionary and for quantizing the images to create histograms based on this dictionary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bow_dictionary(descriptors, n_clusters=100, descriptor_size=128):\n",
    "    # Filter out descriptors that do not match the expected descriptor size\n",
    "    valid_descriptors = []\n",
    "\n",
    "    for d in descriptors:\n",
    "        try:\n",
    "            if d.shape[1] == descriptor_size:\n",
    "                valid_descriptors.append(d)\n",
    "\n",
    "            else:\n",
    "                print(f\"Invalid descriptor size: {d.shape[1]}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing descriptor: {e}\")\n",
    "\n",
    "    # If there are no valid descriptors, return an empty array or handle the case appropriately\n",
    "    if not valid_descriptors:\n",
    "        raise ValueError(\"No valid descriptors found.\")\n",
    "\n",
    "    # Flatten the list of descriptors to fit KMeans\n",
    "    all_descriptors = np.vstack(valid_descriptors)\n",
    "\n",
    "    # Clustering using KMeans\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    kmeans.fit(all_descriptors)\n",
    "\n",
    "    # The cluster centers are our visual vocabulary\n",
    "    bow_dictionary = kmeans.cluster_centers_\n",
    "\n",
    "    return bow_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantize_features(all_bovw, centers):\n",
    "    # Initialize the feature vector\n",
    "    features = np.zeros((len(all_bovw), len(centers)))\n",
    "\n",
    "    # Loop through each BoVW representation\n",
    "    for i, bovw in enumerate(all_bovw):\n",
    "        # Compute the distances to each cluster center\n",
    "        distances = np.linalg.norm(bovw - centers[:, np.newaxis], axis=2)\n",
    "\n",
    "        # Assign the feature to the closest cluster\n",
    "        features[i] = np.sum(distances == np.min(distances, axis=0), axis=1)\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create BoW Dictionary from Extracted Features\n",
    "\n",
    "Using the descriptors extracted from the images in our dataset, we create the BoW dictionary.\n",
    "\n",
    "### Quantize Features of Each Image\n",
    "\n",
    "After creating the BoW dictionary, we quantize the features of each image in our dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sift_descriptors = [\n",
    "    extract_features_sift(os.path.join(DATASET_PATH, imagenette_dir, path))[1]\n",
    "    for path in df_imagenette_paths[\"path\"][:100]\n",
    "]\n",
    "\n",
    "# Create a BoW dictionary using SIFT descriptors\n",
    "bow_dictionary_sift = create_bow_dictionary(sift_descriptors, n_clusters=100)\n",
    "\n",
    "# Quantize the SIFT descriptors using the BoW dictionary\n",
    "quantized_features_sift = quantize_features(sift_descriptors, bow_dictionary_sift)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orb_descriptors = [\n",
    "    extract_features_orb(os.path.join(DATASET_PATH, imagenette_dir, path))[1]\n",
    "    for path in df_imagenette_paths[\"path\"][:100]\n",
    "]\n",
    "\n",
    "# Create a BoW dictionary using ORB descriptors\n",
    "bow_dictionary_orb = create_bow_dictionary(orb_descriptors, n_clusters=100)\n",
    "\n",
    "# Quantize the ORB descriptors using the BoW dictionary\n",
    "quantized_features_orb = quantize_features(orb_descriptors, bow_dictionary_orb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification\n",
    "\n",
    "After forming the Bag of Words (BoW) model for our images, the next step is to classify the images based on their BoW histograms. This section covers the implementation of the k-NN (k-Nearest Neighbors) algorithm for image classification and the evaluation of its performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Functions for Classification\n",
    "\n",
    "We define functions for training the k-NN classifier, classifying images, and evaluating the classifier's performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_knn_classifier(features, labels, n_neighbors=10):\n",
    "    # Initialize the k-NN classifier\n",
    "    knn = KNeighborsClassifier(n_neighbors=n_neighbors)\n",
    "\n",
    "    # Train the classifier\n",
    "    knn.fit(features, labels)\n",
    "\n",
    "    return knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_images(classifier, test_features):\n",
    "    # Predict the labels for the test features\n",
    "    predictions = classifier.predict(test_features)\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_classifier(predictions, true_labels):\n",
    "    # Generate a classification report\n",
    "    report = classification_report(true_labels, predictions)\n",
    "\n",
    "    # Generate a confusion matrix\n",
    "    confusion_mat = confusion_matrix(true_labels, predictions)\n",
    "\n",
    "    return report, confusion_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the k-NN Classifier\n",
    "\n",
    "Using the quantized features and their corresponding labels, we train the k-NN classifier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder for training features and labels\n",
    "# train_features = ...\n",
    "# train_labels = ...\n",
    "\n",
    "# Placeholder for displaying that the classifier has been trained\n",
    "print(\"k-NN classifier trained.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classify Images and Evaluate Performance\n",
    "\n",
    "We classify the test images using the trained classifier and evaluate its performance by comparing the predicted labels with the true labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder for test features and true labels\n",
    "# test_features = ...\n",
    "# true_labels = ...\n",
    "\n",
    "# Classify the test images\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results and Discussion\n",
    "\n",
    "This section presents the results obtained from the classification process and discusses the findings. We will look at feature points for example images, runtime and visual comparison of description methods (SIFT/SURF and ORB), and the related confusion matrices and comparison tables.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Runtime and Visual Comparison\n",
    "\n",
    "We compare the runtime and visual quality of the SIFT/SURF and ORB keypoint detection methods. For brevity, this section will include placeholders for actual runtime data and visual comparisons.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrices and Comparison Tables\n",
    "\n",
    "This subsection includes placeholders for confusion matrices and comparison tables between the different techniques employed in the assignment. The tables aim to highlight the differences in classification accuracy, runtime, and other metrics of interest.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "\n",
    "The discussion focuses on the implications of the findings, including an analysis of the performance differences between the SIFT/SURF and ORB methods, the impact of different distance measures on classification accuracy, and the overall effectiveness of the Bag of Visual Words model for image classification. Insights gained from the comparison tables and confusion matrices are also discussed here, providing a comprehensive overview of the project's outcomes.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyterenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
