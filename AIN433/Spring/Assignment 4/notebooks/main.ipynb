{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ahmet Emre Usta\n",
    "\n",
    "## 2200765036\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Project Link: https://drive.google.com/drive/folders/1_HckxWnghCSo2FFWA-91dsmYUyMBJsVP?usp=sharing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import time\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as torchmodels\n",
    "from PIL import Image\n",
    "from pytorchyolo import models\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from torchsummary import summary\n",
    "from torchvision import transforms\n",
    "from torchvision.models import EfficientNet_B0_Weights\n",
    "from tqdm import tqdm\n",
    "\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_logger(filename):\n",
    "    logging.basicConfig(\n",
    "        filename=os.path.join(\"logs\", filename),\n",
    "        level=logging.INFO,\n",
    "        format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    )\n",
    "    return logging.getLogger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GunsObjectDetectionDataset(Dataset):\n",
    "    def __init__(self, image_dir, label_dir, transform=None, image_size=(224, 224)):\n",
    "        self.image_dir = image_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.transform = transform\n",
    "        self.image_size = image_size\n",
    "        self.image_filenames = [f for f in os.listdir(image_dir) if f.endswith(\".jpeg\")]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_filename = self.image_filenames[idx]\n",
    "        image_path = os.path.join(self.image_dir, image_filename)\n",
    "        label_path = os.path.join(\n",
    "            self.label_dir, image_filename.replace(\".jpeg\", \".txt\")\n",
    "        )\n",
    "\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        original_size = image.size\n",
    "\n",
    "        boxes = []\n",
    "        labels = []\n",
    "\n",
    "        with open(label_path, \"r\") as f:\n",
    "            num_objects = int(f.readline().strip())\n",
    "            for _ in range(num_objects):\n",
    "                box = list(map(float, f.readline().strip().split()))\n",
    "                x_min, y_min, x_max, y_max = box[0], box[1], box[2], box[3]\n",
    "                boxes.append([x_min, y_min, x_max, y_max])\n",
    "                labels.append(1)  # Assuming 1 is the class label for 'gun'\n",
    "\n",
    "        # If there are no objects, set labels to 0 (background)\n",
    "        if len(boxes) == 0:\n",
    "            labels.append(0)\n",
    "            boxes.append([0, 0, 0, 0])\n",
    "\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "\n",
    "        if self.transform:\n",
    "            image, boxes = self.apply_transforms(image, boxes, original_size)\n",
    "\n",
    "        target = {\"boxes\": boxes, \"labels\": labels}\n",
    "\n",
    "        return image, target\n",
    "\n",
    "    def apply_transforms(self, image, boxes, original_size):\n",
    "        if isinstance(self.transform, transforms.Compose):\n",
    "            for t in self.transform.transforms:\n",
    "                if isinstance(t, transforms.Resize):\n",
    "                    image = t(image)\n",
    "                    resized_size = image.size\n",
    "                    scale_x = resized_size[0] / original_size[0]\n",
    "                    scale_y = resized_size[1] / original_size[1]\n",
    "                    boxes[:, [0, 2]] *= scale_x\n",
    "                    boxes[:, [1, 3]] *= scale_y\n",
    "                elif isinstance(t, transforms.RandomHorizontalFlip):\n",
    "                    if torch.rand(1) < 0.5:\n",
    "                        image = t(image)\n",
    "                        width = image.size[0]\n",
    "                        boxes[:, [0, 2]] = width - boxes[:, [2, 0]]\n",
    "                elif isinstance(t, transforms.RandomCrop):\n",
    "                    i, j, h, w = transforms.RandomCrop.get_params(\n",
    "                        image, output_size=self.image_size\n",
    "                    )\n",
    "                    image = transforms.functional.crop(image, i, j, h, w)\n",
    "                    boxes[:, [0, 2]] -= j\n",
    "                    boxes[:, [1, 3]] -= i\n",
    "                    boxes[:, 0] = boxes[:, 0].clamp(min=0, max=w)\n",
    "                    boxes[:, 2] = boxes[:, 2].clamp(min=0, max=w)\n",
    "                    boxes[:, 1] = boxes[:, 1].clamp(min=0, max=h)\n",
    "                    boxes[:, 3] = boxes[:, 3].clamp(min=0, max=h)\n",
    "                elif isinstance(t, transforms.ColorJitter):\n",
    "                    image = t(image)\n",
    "                elif isinstance(t, transforms.ToTensor):\n",
    "                    image = t(image)\n",
    "                elif isinstance(t, transforms.Normalize):\n",
    "                    image = t(image)\n",
    "        else:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_samples(dataset, logger, num_samples=8, max_per_row=4):\n",
    "    num_rows = (num_samples + max_per_row - 1) // max_per_row\n",
    "    fig, axs = plt.subplots(\n",
    "        num_rows, max_per_row, figsize=(max_per_row * 5, num_rows * 5)\n",
    "    )\n",
    "    axs = axs.flatten()\n",
    "\n",
    "    # Mean and std used for normalization in the dataset\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "\n",
    "    # Get random indices using NumPy's random generator\n",
    "    random_indices = np.random.choice(len(dataset), num_samples, replace=False)\n",
    "\n",
    "    for i, idx in enumerate(random_indices):\n",
    "        image, target = dataset[idx]\n",
    "        image = np.array(image).transpose(1, 2, 0)  # Transpose the image\n",
    "\n",
    "        # Unnormalize the image\n",
    "        image = (image * std) + mean\n",
    "        image = np.clip(image, 0, 1)  # Clip to [0, 1] range\n",
    "\n",
    "        axs[i].imshow(image)\n",
    "        for box, label in zip(target[\"boxes\"], target[\"labels\"]):\n",
    "            x_min, y_min, x_max, y_max = box\n",
    "            rect = patches.Rectangle(\n",
    "                (x_min, y_min),\n",
    "                x_max - x_min,\n",
    "                y_max - y_min,\n",
    "                linewidth=1,\n",
    "                edgecolor=\"r\",\n",
    "                facecolor=\"none\",\n",
    "            )\n",
    "            axs[i].add_patch(rect)\n",
    "            axs[i].text(\n",
    "                x_min,\n",
    "                y_min,\n",
    "                f\"Class {label}\",\n",
    "                color=\"white\",\n",
    "                fontsize=8,\n",
    "                bbox=dict(facecolor=\"red\", alpha=0.5),\n",
    "            )\n",
    "\n",
    "        axs[i].axis(\"off\")\n",
    "\n",
    "    # Hide any unused subplots\n",
    "    for j in range(i + 1, len(axs)):\n",
    "        axs[j].axis(\"off\")\n",
    "\n",
    "    fig.suptitle(\"Dataset Samples\", fontsize=16, fontweight=\"bold\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(\"graphs\", \"dataset_samples.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    # Print a message to indicate that the samples are shown\n",
    "    logger.info(\"Samples are shown and saved to 'graphs' directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(\n",
    "    cls_out, reg_out, cls_targets, reg_targets, criterion_cls, criterion_reg\n",
    "):\n",
    "    cls_loss = criterion_cls(cls_out, cls_targets)\n",
    "    reg_loss = criterion_reg(reg_out, reg_targets)\n",
    "    loss = cls_loss + reg_loss\n",
    "    return loss, cls_loss, reg_loss\n",
    "\n",
    "\n",
    "def compute_iou(pred_boxes, true_boxes):\n",
    "    # Intersection over Union (IoU) calculation\n",
    "    inter_xmin = torch.max(pred_boxes[:, 0], true_boxes[:, 0])\n",
    "    inter_ymin = torch.max(pred_boxes[:, 1], true_boxes[:, 1])\n",
    "    inter_xmax = torch.min(pred_boxes[:, 2], true_boxes[:, 2])\n",
    "    inter_ymax = torch.min(pred_boxes[:, 3], true_boxes[:, 3])\n",
    "\n",
    "    inter_area = torch.clamp(inter_xmax - inter_xmin, min=0) * torch.clamp(\n",
    "        inter_ymax - inter_ymin, min=0\n",
    "    )\n",
    "    pred_area = (pred_boxes[:, 2] - pred_boxes[:, 0]) * (\n",
    "        pred_boxes[:, 3] - pred_boxes[:, 1]\n",
    "    )\n",
    "    true_area = (true_boxes[:, 2] - true_boxes[:, 0]) * (\n",
    "        true_boxes[:, 3] - true_boxes[:, 1]\n",
    "    )\n",
    "\n",
    "    union_area = pred_area + true_area - inter_area\n",
    "    iou = inter_area / union_area\n",
    "\n",
    "    return iou.mean().item()\n",
    "\n",
    "\n",
    "def compute_iou_np(boxA, boxB):\n",
    "    xA = max(boxA[0], boxB[0])\n",
    "    yA = max(boxA[1], boxB[1])\n",
    "    xB = min(boxA[2], boxB[2])\n",
    "    yB = min(boxA[3], boxB[3])\n",
    "\n",
    "    interArea = max(0, xB - xA + 1) * max(0, yB - yA + 1)\n",
    "    boxAArea = (boxA[2] - boxA[0] + 1) * (boxA[3] - boxA[1] + 1)\n",
    "    boxBArea = (boxB[2] - boxB[0] + 1) * (boxB[3] - boxB[1] + 1)\n",
    "\n",
    "    iou = interArea / float(boxAArea + boxBArea - interArea)\n",
    "    return iou\n",
    "\n",
    "\n",
    "def non_max_suppression(boxes, scores, iou_threshold=0.5):\n",
    "    if len(boxes) == 0:\n",
    "        return []\n",
    "\n",
    "    boxes = np.array(boxes)\n",
    "    scores = np.array(scores)\n",
    "\n",
    "    x1 = boxes[:, 0]\n",
    "    y1 = boxes[:, 1]\n",
    "    x2 = boxes[:, 2]\n",
    "    y2 = boxes[:, 3]\n",
    "\n",
    "    areas = (x2 - x1 + 1) * (y2 - y1 + 1)\n",
    "    order = scores.argsort()[::-1]\n",
    "\n",
    "    keep = []\n",
    "    while order.size > 0:\n",
    "        i = order[0]\n",
    "        keep.append(i)\n",
    "\n",
    "        xx1 = np.maximum(x1[i], x1[order[1:]])\n",
    "        yy1 = np.maximum(y1[i], y1[order[1:]])\n",
    "        xx2 = np.minimum(x2[i], x2[order[1:]])\n",
    "        yy2 = np.minimum(y2[i], y2[order[1:]])\n",
    "\n",
    "        w = np.maximum(0, xx2 - xx1 + 1)\n",
    "        h = np.maximum(0, yy2 - yy1 + 1)\n",
    "        inter = w * h\n",
    "        ovr = inter / (areas[i] + areas[order[1:]] - inter)\n",
    "\n",
    "        inds = np.where(ovr <= iou_threshold)[0]\n",
    "        order = order[inds + 1]\n",
    "\n",
    "    return keep\n",
    "\n",
    "\n",
    "def sliding_window(image, window_size, stride):\n",
    "    \"\"\"Generate sliding windows for the given image.\"\"\"\n",
    "    windows = []\n",
    "    _, height, width = image.shape\n",
    "\n",
    "    for y in range(0, height - window_size + 1, stride):\n",
    "        for x in range(0, width - window_size + 1, stride):\n",
    "            window = image[:, y : y + window_size, x : x + window_size]\n",
    "            windows.append((window, (x, y)))\n",
    "\n",
    "    return windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OverfeatWithEfficientNet(nn.Module):\n",
    "    def __init__(self, debug=False):\n",
    "        super(OverfeatWithEfficientNet, self).__init__()\n",
    "        self.debug = debug\n",
    "\n",
    "        # Load the pre-trained EfficientNetB0\n",
    "        efficientnet = torchmodels.efficientnet_b0(\n",
    "            weights=EfficientNet_B0_Weights.DEFAULT\n",
    "        )\n",
    "\n",
    "        # Freeze the parameters of the pre-trained layers\n",
    "        for param in efficientnet.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Use the EfficientNet up to the last convolutional layer\n",
    "        self.backbone = nn.Sequential(*list(efficientnet.children())[:-1])\n",
    "\n",
    "        # Flatten the output of the backbone\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        # Classification head with more layers\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(1280, 512),  # Updated input size\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 2),  # 2 classes: background and gun\n",
    "        )\n",
    "\n",
    "        # Regression head for bounding box with more layers\n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.Linear(1280, 512),  # Updated input size\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 4),  # 4 coordinates: x_min, y_min, x_max, y_max\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.debug:\n",
    "            print(\"Input shape:\", x.shape)\n",
    "\n",
    "        features = self.backbone(x)\n",
    "\n",
    "        if self.debug:\n",
    "            print(\"Features shape after backbone:\", features.shape)\n",
    "\n",
    "        features = self.flatten(features)\n",
    "\n",
    "        if self.debug:\n",
    "            print(\"Features shape after flattening:\", features.shape)\n",
    "\n",
    "        cls_out = self.classifier(features)\n",
    "\n",
    "        if self.debug:\n",
    "            print(\"Classification output shape:\", cls_out.shape)\n",
    "\n",
    "        reg_out = self.regressor(features)\n",
    "\n",
    "        if self.debug:\n",
    "            print(\"Regression output shape:\", reg_out.shape)\n",
    "\n",
    "        return cls_out, reg_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YOLOv3TinyModel(nn.Module):\n",
    "    def __init__(self, num_classes=2, debug=False):\n",
    "        super(YOLOv3TinyModel, self).__init__()\n",
    "        self.debug = debug\n",
    "\n",
    "        # Load the pre-trained YOLOv3-tiny model using pytorchyolo\n",
    "        yolo_v3_tiny = models.load_model(\n",
    "            r\"C:\\Users\\aliseydi\\Git\\Assignment 4\\models\\pretrained_weights\\PyTorch-YOLOv3/config/yolov3-tiny.cfg\",\n",
    "            r\"C:\\Users\\aliseydi\\Git\\Assignment 4\\models\\pretrained_weights\\PyTorch-YOLOv3/weights/yolov3-tiny.weights\",\n",
    "        )\n",
    "\n",
    "        # Freeze all layers except the last three layers\n",
    "        for param in list(yolo_v3_tiny.parameters())[:-3]:\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Modify the last layer to accommodate the number of classes required\n",
    "        yolo_v3_tiny.module_list[-1] = nn.Conv2d(\n",
    "            in_channels=255, out_channels=num_classes + 5, kernel_size=1\n",
    "        )\n",
    "\n",
    "        self.backbone = nn.Sequential(*list(yolo_v3_tiny.module_list[:-1]))\n",
    "        self.head = yolo_v3_tiny.module_list[-1]\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        # Classification head with more layers\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, num_classes),\n",
    "        )\n",
    "\n",
    "        # Regression head for bounding box with more layers\n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 4),  # 4 coordinates: x_min, y_min, x_max, y_max\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.debug:\n",
    "            print(\"Input shape:\", x.shape)\n",
    "\n",
    "        features = self.backbone(x)\n",
    "\n",
    "        if self.debug:\n",
    "            print(\"Features shape after backbone:\", features.shape)\n",
    "\n",
    "        features = self.flatten(features)\n",
    "\n",
    "        if self.debug:\n",
    "            print(\"Features shape after flattening:\", features.shape)\n",
    "\n",
    "        cls_out = self.classifier(features)\n",
    "\n",
    "        if self.debug:\n",
    "            print(\"Classification output shape:\", cls_out.shape)\n",
    "\n",
    "        reg_out = self.regressor(features)\n",
    "\n",
    "        if self.debug:\n",
    "            print(\"Regression output shape:\", reg_out.shape)\n",
    "\n",
    "        return cls_out, reg_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    epochs=10,\n",
    "    learning_rate=1e-4,\n",
    "    device=\"cuda\",\n",
    "    logger=None,\n",
    "    debug=False,\n",
    "):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion_cls = nn.CrossEntropyLoss().to(device)\n",
    "    criterion_reg = nn.MSELoss().to(device)\n",
    "\n",
    "    history = defaultdict(list)\n",
    "    best_loss = float(\"inf\")\n",
    "    best_model_wts = model.state_dict()\n",
    "    start_time = time.time()\n",
    "\n",
    "    def compute_metrics(loader, is_train=True):\n",
    "        model.train() if is_train else model.eval()\n",
    "        total_loss, total_cls_loss, total_reg_loss = 0, 0, 0\n",
    "        total_correct, total_cls_samples, total_iou = 0, 0, 0\n",
    "\n",
    "        for images, targets in loader:\n",
    "            images = torch.stack(images).to(device)\n",
    "            cls_targets = torch.cat([t[\"labels\"] for t in targets], dim=0).to(device)\n",
    "            reg_targets = torch.cat([t[\"boxes\"] for t in targets], dim=0).to(device)\n",
    "\n",
    "            if is_train:\n",
    "                optimizer.zero_grad()\n",
    "                cls_out, reg_out = model(images)\n",
    "                cls_targets = cls_targets[: cls_out.size(0)]\n",
    "                reg_targets = reg_targets[: reg_out.size(0)]\n",
    "                total_loss, cls_loss, reg_loss = compute_loss(\n",
    "                    cls_out,\n",
    "                    reg_out,\n",
    "                    cls_targets,\n",
    "                    reg_targets,\n",
    "                    criterion_cls,\n",
    "                    criterion_reg,\n",
    "                )\n",
    "                total_loss.backward()\n",
    "                optimizer.step()\n",
    "                # convert total loss tensor to numpy cpu\n",
    "                total_loss = total_loss.cpu().detach().numpy()\n",
    "\n",
    "            else:\n",
    "                with torch.no_grad():\n",
    "                    cls_out, reg_out = model(images)\n",
    "                    cls_targets = cls_targets[: cls_out.size(0)]\n",
    "                    reg_targets = reg_targets[: reg_out.size(0)]\n",
    "                    total_loss, cls_loss, reg_loss = compute_loss(\n",
    "                        cls_out,\n",
    "                        reg_out,\n",
    "                        cls_targets,\n",
    "                        reg_targets,\n",
    "                        criterion_cls,\n",
    "                        criterion_reg,\n",
    "                    )\n",
    "\n",
    "            total_loss += total_loss.item()\n",
    "            total_cls_loss += cls_loss.item()\n",
    "            total_reg_loss += reg_loss.item()\n",
    "            _, preds = torch.max(cls_out, 1)\n",
    "            total_correct += (preds == cls_targets).sum().item()\n",
    "            total_cls_samples += cls_targets.size(0)\n",
    "            total_iou += compute_iou(reg_out, reg_targets)\n",
    "\n",
    "        avg_loss = total_loss / len(loader)\n",
    "        avg_cls_loss = total_cls_loss / len(loader)\n",
    "        avg_reg_loss = total_reg_loss / len(loader)\n",
    "        avg_accuracy = total_correct / total_cls_samples\n",
    "        avg_iou = total_iou / len(loader)\n",
    "\n",
    "        if debug:\n",
    "            print(\n",
    "                type(avg_loss),\n",
    "                type(avg_cls_loss),\n",
    "                type(avg_reg_loss),\n",
    "                type(avg_accuracy),\n",
    "                type(avg_iou),\n",
    "            )\n",
    "\n",
    "        return (\n",
    "            avg_loss,\n",
    "            avg_cls_loss,\n",
    "            avg_reg_loss,\n",
    "            avg_accuracy,\n",
    "            avg_iou,\n",
    "        )\n",
    "\n",
    "    # use tqdm for progress bar\n",
    "    for epoch in tqdm(range(epochs), desc=\"Epochs\", unit=\"epoch\"):\n",
    "        train_metrics = compute_metrics(train_loader, is_train=True)\n",
    "        val_metrics = compute_metrics(val_loader, is_train=False)\n",
    "\n",
    "        # Training metrics\n",
    "        history[\"train_loss\"].append(train_metrics[0])\n",
    "        history[\"train_cls_loss\"].append(train_metrics[1])\n",
    "        history[\"train_reg_loss\"].append(train_metrics[2])\n",
    "        history[\"train_accuracy\"].append(train_metrics[3])\n",
    "        history[\"train_iou\"].append(train_metrics[4])\n",
    "\n",
    "        # Validation metrics\n",
    "        history[\"val_loss\"].append(val_metrics[0])\n",
    "        history[\"val_cls_loss\"].append(val_metrics[1])\n",
    "        history[\"val_reg_loss\"].append(val_metrics[2])\n",
    "        history[\"val_accuracy\"].append(val_metrics[3])\n",
    "        history[\"val_iou\"].append(val_metrics[4])\n",
    "\n",
    "        if val_metrics[0] < best_loss:\n",
    "            best_loss = val_metrics[0]\n",
    "            best_model_wts = model.state_dict()\n",
    "            torch.save(best_model_wts, os.path.join(\"models\", \"best_model.pth\"))\n",
    "\n",
    "        epoch_time = time.time() - start_time\n",
    "        logger.info(\n",
    "            f\"Epoch [{epoch+1}/{epochs}], Train Loss: {train_metrics[0]:.4f}, Val Loss: {val_metrics[0]:.4f}, \"\n",
    "            f\"Train CLS Loss: {train_metrics[1]:.4f}, Val CLS Loss: {val_metrics[1]:.4f}, Train REG Loss: {train_metrics[2]:.4f}, \"\n",
    "            f\"Val REG Loss: {val_metrics[2]:.4f}, Train Accuracy: {train_metrics[3]:.4f}, Val Accuracy: {val_metrics[3]:.4f}, \"\n",
    "            f\"Train mIoU: {train_metrics[4]:.4f}, Val mIoU: {val_metrics[4]:.4f}, Time: {epoch_time:.2f}s\"\n",
    "        )\n",
    "\n",
    "    # close tqdm with message\n",
    "    tqdm.write(\"Training completed.\")\n",
    "\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    total_time = time.time() - start_time\n",
    "    logger.info(f\"Training completed in {total_time // 60:.0f}m {total_time % 60:.0f}s\")\n",
    "    logger.info(f\"Best validation loss: {best_loss:.4f}\")\n",
    "\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history, logger=None, debug=False):\n",
    "    if debug:\n",
    "        for k, v in history.items():\n",
    "            print(k, type(v), len(v))\n",
    "            # print random 5 elements\n",
    "            if isinstance(v, list):\n",
    "                print(v[:5])\n",
    "\n",
    "    # Convert CUDA tensors to CPU tensors and then to numpy if necessary\n",
    "    for k, v in history.items():\n",
    "        if isinstance(v, torch.Tensor):\n",
    "            history[k] = v.cpu().numpy()\n",
    "        elif isinstance(v, list) and isinstance(v[0], torch.Tensor):\n",
    "            history[k] = [t.cpu().numpy() for t in v]\n",
    "\n",
    "    epochs = range(1, len(history[\"train_loss\"]) + 1)\n",
    "\n",
    "    plt.figure(figsize=(18, 12))\n",
    "\n",
    "    # Plot Training and Validation Loss\n",
    "    plt.subplot(2, 3, 1)\n",
    "    plt.plot(epochs, history[\"train_loss\"], label=\"Train Loss\", color=\"blue\")\n",
    "    plt.plot(epochs, history[\"val_loss\"], label=\"Val Loss\", color=\"orange\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Training and Validation Loss\")\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot Training and Validation Classification Loss\n",
    "    plt.subplot(2, 3, 2)\n",
    "    plt.plot(epochs, history[\"train_cls_loss\"], label=\"Train CLS Loss\", color=\"blue\")\n",
    "    plt.plot(epochs, history[\"val_cls_loss\"], label=\"Val CLS Loss\", color=\"orange\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"CLS Loss\")\n",
    "    plt.title(\"Training and Validation Classification Loss\")\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot Training and Validation Regression Loss\n",
    "    plt.subplot(2, 3, 3)\n",
    "    plt.plot(epochs, history[\"train_reg_loss\"], label=\"Train REG Loss\", color=\"blue\")\n",
    "    plt.plot(epochs, history[\"val_reg_loss\"], label=\"Val REG Loss\", color=\"orange\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"REG Loss\")\n",
    "    plt.title(\"Training and Validation Regression Loss\")\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot Training and Validation Accuracy\n",
    "    plt.subplot(2, 3, 4)\n",
    "    plt.plot(epochs, history[\"train_accuracy\"], label=\"Train Accuracy\", color=\"blue\")\n",
    "    plt.plot(epochs, history[\"val_accuracy\"], label=\"Val Accuracy\", color=\"orange\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.title(\"Training and Validation Accuracy\")\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot Training and Validation mIoU\n",
    "    plt.subplot(2, 3, 5)\n",
    "    plt.plot(epochs, history[\"train_iou\"], label=\"Train mIoU\", color=\"blue\")\n",
    "    plt.plot(epochs, history[\"val_iou\"], label=\"Val mIoU\", color=\"orange\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"mIoU\")\n",
    "    plt.title(\"Training and Validation mIoU\")\n",
    "    plt.legend()\n",
    "\n",
    "    # Add bold and big title\n",
    "    plt.suptitle(\"Training History\", fontsize=16, fontweight=\"bold\")\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    plt.savefig(os.path.join(\"graphs\", \"training_history.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    if logger:\n",
    "        logger.info(\"Training history plot is saved to 'graphs' directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataloader, window_size, stride, iou_threshold, device):\n",
    "    \"\"\"Evaluate the model using sliding window and non-max suppression.\"\"\"\n",
    "    model.eval()\n",
    "    total_iou = 0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    all_predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, targets in dataloader:\n",
    "            images = torch.stack(images).to(device)\n",
    "            batch_predictions = []\n",
    "\n",
    "            for image, target in zip(images, targets):\n",
    "                windows = sliding_window(image, window_size, stride)\n",
    "                boxes = []\n",
    "                scores = []\n",
    "\n",
    "                for window, (x, y) in windows:\n",
    "                    window = window.unsqueeze(0).to(device)\n",
    "                    outputs = model(window)\n",
    "\n",
    "                    # Classification and Regression outputs\n",
    "                    class_scores = F.softmax(outputs[0], dim=1)\n",
    "                    bbox_preds = outputs[1]\n",
    "\n",
    "                    max_score, predicted_class = torch.max(class_scores, dim=1)\n",
    "\n",
    "                    if max_score.item() > 0.5:  # Threshold score\n",
    "                        boxes.append(\n",
    "                            [\n",
    "                                x + bbox_preds[0][0].item(),\n",
    "                                y + bbox_preds[0][1].item(),\n",
    "                                x + bbox_preds[0][2].item(),\n",
    "                                y + bbox_preds[0][3].item(),\n",
    "                            ]\n",
    "                        )\n",
    "                        scores.append(max_score.item())\n",
    "\n",
    "                if boxes:\n",
    "                    keep = non_max_suppression(boxes, scores, iou_threshold)\n",
    "                    boxes = [boxes[i] for i in keep]\n",
    "                    scores = [scores[i] for i in keep]\n",
    "\n",
    "                    batch_predictions.append({\"boxes\": boxes, \"scores\": scores})\n",
    "\n",
    "                    # Calculate mIoU\n",
    "                    gt_boxes = target[\"boxes\"].cpu().numpy()\n",
    "                    if len(boxes) > 0 and len(gt_boxes) > 0:\n",
    "                        ious = [\n",
    "                            compute_iou_np(pred_box, gt_box)\n",
    "                            for pred_box in boxes\n",
    "                            for gt_box in gt_boxes\n",
    "                        ]\n",
    "                        total_iou += sum(ious) / len(ious)\n",
    "                        total_correct += sum(1 for iou in ious if iou > 0.5)\n",
    "                        total_samples += len(gt_boxes)\n",
    "\n",
    "            all_predictions.append(batch_predictions)\n",
    "\n",
    "    mIoU = total_iou / total_samples if total_samples > 0 else 0\n",
    "    accuracy = total_correct / total_samples if total_samples > 0 else 0\n",
    "\n",
    "    return mIoU, accuracy, all_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_evaluation_results(\n",
    "    test_loader,\n",
    "    predictions,\n",
    "    mIoU,\n",
    "    accuracy,\n",
    "    max_per_row=4,\n",
    "    logger=None,\n",
    "    mean=[0.485, 0.456, 0.406],\n",
    "    std=[0.229, 0.224, 0.225],\n",
    "):\n",
    "    \"\"\"\n",
    "    Visualize the evaluation results and display mIoU and accuracy.\n",
    "\n",
    "    Parameters:\n",
    "    - test_loader: DataLoader containing the test dataset.\n",
    "    - predictions: List of predictions containing boxes and scores.\n",
    "    - mIoU: Mean Intersection over Union.\n",
    "    - accuracy: Classification accuracy.\n",
    "    - max_per_row: Maximum number of images to display in a row.\n",
    "    - mean: Mean used for normalization.\n",
    "    - std: Standard deviation used for normalization.\n",
    "    \"\"\"\n",
    "    images, targets = [], []\n",
    "    for images_batch, targets_batch in test_loader:\n",
    "        images.extend(images_batch)\n",
    "        targets.extend(targets_batch)\n",
    "\n",
    "    # extend predictions too\n",
    "    predictions = [pred for batch in predictions for pred in batch]\n",
    "\n",
    "    num_samples = len(images)\n",
    "    num_rows = (num_samples + max_per_row - 1) // max_per_row\n",
    "\n",
    "    fig, axs = plt.subplots(\n",
    "        num_rows, max_per_row, figsize=(max_per_row * 5, num_rows * 5)\n",
    "    )\n",
    "    axs = axs.flatten()\n",
    "\n",
    "    # print(len(targets), len(predictions))\n",
    "\n",
    "    for i, (image, target, prediction) in enumerate(zip(images, targets, predictions)):\n",
    "        if i >= len(axs):\n",
    "            break\n",
    "        ax = axs[i]\n",
    "\n",
    "        # Unnormalize the image\n",
    "        image_np = image.permute(1, 2, 0).cpu().numpy()\n",
    "        image_np = image_np * std + mean\n",
    "        image_np = (image_np * 255).astype(\"uint8\")\n",
    "\n",
    "        ax.imshow(image_np)\n",
    "\n",
    "        # Plot ground truth boxes\n",
    "        for box in target[\"boxes\"].cpu().numpy():\n",
    "            x1, y1, x2, y2 = box\n",
    "            width, height = x2 - x1, y2 - y1\n",
    "            rect = patches.Rectangle(\n",
    "                (x1, y1),\n",
    "                width,\n",
    "                height,\n",
    "                linewidth=2,\n",
    "                edgecolor=\"green\",\n",
    "                facecolor=\"none\",\n",
    "            )\n",
    "            ax.add_patch(rect)\n",
    "            ax.text(\n",
    "                x1,\n",
    "                y1 - 10,\n",
    "                \"Ground Truth\",\n",
    "                color=\"green\",\n",
    "                fontsize=9,\n",
    "                fontweight=\"bold\",\n",
    "                bbox=dict(facecolor=\"white\", alpha=0.5),\n",
    "            )\n",
    "\n",
    "        for box, score in zip(prediction[\"boxes\"], prediction[\"scores\"]):\n",
    "            x1, y1, x2, y2 = box\n",
    "            width, height = x2 - x1, y2 - y1\n",
    "            rect = patches.Rectangle(\n",
    "                (x1, y1),\n",
    "                width,\n",
    "                height,\n",
    "                linewidth=2,\n",
    "                edgecolor=\"red\",\n",
    "                facecolor=\"none\",\n",
    "            )\n",
    "            ax.add_patch(rect)\n",
    "            ax.text(\n",
    "                x1,\n",
    "                y1 - 30,\n",
    "                f\"{score:.2f}\",\n",
    "                color=\"red\",\n",
    "                fontsize=9,\n",
    "                bbox=dict(facecolor=\"white\", alpha=0.5),\n",
    "            )\n",
    "\n",
    "        ax.set_title(f\"Evaluation Result {i + 1}\")\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "    # Hide any unused subplots\n",
    "    for j in range(i + 1, len(axs)):\n",
    "        axs[j].axis(\"off\")\n",
    "\n",
    "    # Display mIoU and accuracy\n",
    "    fig.suptitle(\n",
    "        f\"Evaluation Results\\nmIoU: {mIoU:.4f}, Accuracy: {accuracy:.4f}\",\n",
    "        fontsize=16,\n",
    "        fontweight=\"bold\",\n",
    "    )\n",
    "\n",
    "    if not os.path.exists(\"graphs\"):\n",
    "        os.makedirs(\"graphs\")\n",
    "\n",
    "    plt.savefig(os.path.join(\"graphs\", \"evaluation_results.png\"))\n",
    "\n",
    "    if logger:\n",
    "        logger.info(\"Evaluation results are saved to 'graphs' directory.\")\n",
    "\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_metrics(mIoU, accuracy):\n",
    "    \"\"\"\n",
    "    Visualizes the mIoU and accuracy metrics using bar plots.\n",
    "\n",
    "    Parameters:\n",
    "    mIoU (float): Mean Intersection over Union.\n",
    "    accuracy (float): Accuracy of the model.\n",
    "    \"\"\"\n",
    "\n",
    "    # Data for plotting\n",
    "    metrics = [\"Mean IoU\", \"Accuracy\"]\n",
    "    values = [mIoU, accuracy]\n",
    "\n",
    "    # Create bar plot\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.bar(metrics, values, color=[\"blue\", \"green\"])\n",
    "\n",
    "    # Add title and labels\n",
    "    plt.title(\"Model Evaluation Metrics\")\n",
    "    plt.xlabel(\"Metrics\")\n",
    "    plt.ylabel(\"Values\")\n",
    "\n",
    "    # Add value labels on top of bars\n",
    "    for i in range(len(metrics)):\n",
    "        plt.text(i, values[i] + 0.01, f\"{values[i]:.4f}\", ha=\"center\", va=\"bottom\")\n",
    "\n",
    "    # Display the plot\n",
    "    plt.ylim(0, 1)\n",
    "\n",
    "    # add bold and big titl\n",
    "    plt.suptitle(\"Model Evaluation Metrics\", fontsize=16, fontweight=\"bold\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save the plot\n",
    "    plt.savefig(os.path.join(\"graphs\", \"evaluation_metrics.png\"))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(debug=False):\n",
    "    # Create necessary directories\n",
    "    os.makedirs(\"graphs\", exist_ok=True)\n",
    "    os.makedirs(\"models\", exist_ok=True)\n",
    "    os.makedirs(\"logs\", exist_ok=True)\n",
    "\n",
    "    log_filename = f\"log_train_{datetime.now().strftime('%Y%m%d-%H%M%S')}.log\"\n",
    "    logger = create_logger(log_filename)\n",
    "\n",
    "    # Print the logger filename\n",
    "    print(f\"Logging to {log_filename}\")\n",
    "    logger.info(\"Starting the training process.\")\n",
    "\n",
    "    # log created files\n",
    "    logger.info(\"Created directories: 'graphs', 'models', 'logs'\")\n",
    "\n",
    "    # Directory paths\n",
    "    DATASET_DIR = r\"C:\\Users\\aliseydi\\Git\\Assignment 4\\dataset\"\n",
    "    image_dir = os.path.join(DATASET_DIR, \"Images\")\n",
    "    label_dir = os.path.join(DATASET_DIR, \"Labels\")\n",
    "\n",
    "    # print the dataset directory\n",
    "    logger.info(f\"Images readed from {image_dir}, Labels readed from {label_dir}\")\n",
    "\n",
    "    # Set random seed for reproducibility\n",
    "    MANUAL_SEED = 42\n",
    "    torch.manual_seed(MANUAL_SEED)\n",
    "\n",
    "    # log manual seed\n",
    "    logger.info(f\"Set manual seed {MANUAL_SEED} for reproducibility.\")\n",
    "\n",
    "    BATCH_SIZE = 8\n",
    "    EPOCHS = 200\n",
    "    IMAGE_SIZE = (128, 128)\n",
    "\n",
    "    # print the default batch size and epochs\n",
    "    logger.info(\n",
    "        f\"Batch size: {BATCH_SIZE}, Epochs: {EPOCHS}, Image size: {IMAGE_SIZE[0]} x {IMAGE_SIZE[1]}\"\n",
    "    )\n",
    "\n",
    "    # Check if GPU is available\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    # print the device\n",
    "    logger.info(f\"Using {device.type.upper()} for computation.\")\n",
    "\n",
    "    # Improved data transforms for object detection\n",
    "    data_transforms = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize(\n",
    "                (int(IMAGE_SIZE[0] * 1.2), int(IMAGE_SIZE[1] * 1.2))\n",
    "            ),  # Resize to a larger size\n",
    "            transforms.RandomCrop(IMAGE_SIZE),  # Randomly crop to the desired size\n",
    "            transforms.ColorJitter(\n",
    "                brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2\n",
    "            ),  # Random color jitter\n",
    "            # transforms.RandomHorizontalFlip(),  # Randomly flip horizontally\n",
    "            transforms.ToTensor(),  # Convert to tensor\n",
    "            transforms.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
    "            ),  # Normalize\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    test_transforms = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize(IMAGE_SIZE),  # Resize to the desired size\n",
    "            transforms.ToTensor(),  # Convert to tensor\n",
    "            transforms.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
    "            ),  # Normalize\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Create dataset\n",
    "    dataset = GunsObjectDetectionDataset(\n",
    "        image_dir, label_dir, transform=data_transforms, image_size=IMAGE_SIZE\n",
    "    )\n",
    "\n",
    "    # Check if dataset is correctly loaded\n",
    "    logger.info(f\"Dataset length: {len(dataset)}\")\n",
    "\n",
    "    if len(dataset) == 0:\n",
    "        raise ValueError(\"No data found in the dataset. Please check the dataset path.\")\n",
    "\n",
    "    # Show 4 samples\n",
    "    show_samples(dataset, num_samples=12, logger=logger)\n",
    "\n",
    "    # Calculate lengths for train, validation, and test splits\n",
    "    total_size = len(dataset)\n",
    "    train_size = int(0.6 * total_size)\n",
    "    val_size = int(0.2 * total_size)\n",
    "    test_size = total_size - train_size - val_size\n",
    "\n",
    "    # print the sizes of the splits\n",
    "    logger.info(\n",
    "        f\"Train size: {train_size}, Validation size: {val_size}, Test size: {test_size}\"\n",
    "    )\n",
    "\n",
    "    if not os.path.exists(os.path.join(\"graphs\", \"dataset_split.png\")):\n",
    "        # Plot the split sizes\n",
    "        # plot_split_sizes(train_size, val_size, test_size, logger=logger)\n",
    "        pass\n",
    "\n",
    "    # Split the dataset\n",
    "    train_dataset, val_dataset, test_dataset = random_split(\n",
    "        dataset, [train_size, val_size, test_size]\n",
    "    )\n",
    "\n",
    "    # Apply test transforms to the test dataset\n",
    "    test_dataset.dataset.transform = test_transforms\n",
    "\n",
    "    # Define model configurations\n",
    "    model_configs = [\n",
    "        (\"YOLOv3Tiny\", YOLOv3TinyModel, [8, 16], [1e-4, 1e-6]),\n",
    "        (\"EfficientNet\", OverfeatWithEfficientNet, [8, 16], [1e-4, 1e-6]),\n",
    "    ]\n",
    "\n",
    "    for model_name, model_class, batch_sizes, learning_rates in model_configs:\n",
    "        for batch_size in batch_sizes:\n",
    "            for lr in learning_rates:\n",
    "                # Create DataLoaders\n",
    "                train_loader = DataLoader(\n",
    "                    train_dataset,\n",
    "                    batch_size=batch_size,\n",
    "                    shuffle=True,\n",
    "                    collate_fn=lambda x: tuple(zip(*x)),\n",
    "                )\n",
    "                val_loader = DataLoader(\n",
    "                    val_dataset,\n",
    "                    batch_size=batch_size,\n",
    "                    shuffle=False,\n",
    "                    collate_fn=lambda x: tuple(zip(*x)),\n",
    "                )\n",
    "                test_loader = DataLoader(\n",
    "                    test_dataset,\n",
    "                    batch_size=batch_size,\n",
    "                    shuffle=False,\n",
    "                    collate_fn=lambda x: tuple(zip(*x)),\n",
    "                )\n",
    "\n",
    "                # Log batch size and learning rate\n",
    "                logger.info(\n",
    "                    f\"Training {model_name} with batch size: {batch_size}, learning rate: {lr}\"\n",
    "                )\n",
    "\n",
    "                model = model_class(debug=debug).to(device)\n",
    "\n",
    "                if debug:\n",
    "                    summary(model, (3, IMAGE_SIZE[0], IMAGE_SIZE[1]))\n",
    "\n",
    "                # Train the model\n",
    "                best_model, history = train_model(\n",
    "                    model,\n",
    "                    train_loader,\n",
    "                    val_loader,\n",
    "                    epochs=200,\n",
    "                    learning_rate=lr,\n",
    "                    device=device,\n",
    "                    logger=logger,\n",
    "                    debug=debug,\n",
    "                )\n",
    "\n",
    "                # Plot the training history\n",
    "                plot_history(history, logger, debug=debug)\n",
    "\n",
    "                window_size = 64  # Example window size\n",
    "                stride = 16  # Example stride\n",
    "                iou_threshold = 0.5  # Example IoU threshold\n",
    "\n",
    "                mIoU, accuracy, predictions = evaluate_model(\n",
    "                    best_model, test_loader, window_size, stride, iou_threshold, device\n",
    "                )\n",
    "\n",
    "                logger.info(\n",
    "                    f\"{model_name} - Mean IoU: {mIoU:.4f}, Accuracy: {accuracy:.4f}\"\n",
    "                )\n",
    "\n",
    "                # Visualize the evaluation metrics\n",
    "                visualize_metrics(mIoU, accuracy)\n",
    "\n",
    "                # Visualize the evaluation results\n",
    "                visualize_evaluation_results(\n",
    "                    test_loader, predictions, mIoU, accuracy, logger=logger\n",
    "                )\n",
    "\n",
    "                # Save the best model\n",
    "                model_save_path = os.path.join(\n",
    "                    \"models\", f\"{model_name}_bs{batch_size}_lr{lr}.pth\"\n",
    "                )\n",
    "                torch.save(best_model.state_dict(), model_save_path)\n",
    "                logger.info(f\"Saved {model_name} model to {model_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main(debug=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object Detection Using YOLOv3-Tiny and EfficientNet\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "### Problem Statement\n",
    "\n",
    "The task addressed in this project is the detection of guns in images using object detection techniques. Object detection involves both identifying objects within an image and locating them with bounding boxes, which is essential for applications such as security and surveillance systems.\n",
    "\n",
    "### Approach\n",
    "\n",
    "To tackle this problem, we implemented two advanced deep learning models: YOLOv3-Tiny and EfficientNet. YOLOv3-Tiny is known for its balance between speed and accuracy, making it suitable for real-time applications. EfficientNet, with its powerful feature extraction capabilities, offers higher accuracy at the cost of increased computational resources.\n",
    "\n",
    "### Report Content\n",
    "\n",
    "This report is structured as follows:\n",
    "\n",
    "1. **Introduction**: Overview of the problem, approach, and report content.\n",
    "2. **Implementation Details**: Detailed explanation of the methods and solutions used.\n",
    "3. **Experimental Results**: Presentation and analysis of results from different experiments, including various hyperparameters and visualized bounding box predictions.\n",
    "4. **Conclusion**: Summary of the results, discussion of implementation weaknesses, and suggestions for future work.\n",
    "\n",
    "## 2. Implementation Details\n",
    "\n",
    "### Data Preparation\n",
    "\n",
    "The dataset used for this project contains images of guns, with corresponding labels for the bounding boxes. We implemented a custom `GunsObjectDetectionDataset` class to handle image loading, transformation, and augmentation. The dataset was split into training, validation, and test sets with a ratio of 60:20:20.\n",
    "\n",
    "### Model Architectures\n",
    "\n",
    "We used two models:\n",
    "\n",
    "1. **YOLOv3-Tiny**: A lightweight version of YOLOv3, modified to suit our specific problem.\n",
    "2. **EfficientNet**: A pre-trained EfficientNet model, modified for object detection tasks.\n",
    "\n",
    "Both models were modified to include custom classification and regression heads to predict bounding box coordinates and object classes.\n",
    "\n",
    "### Training and Evaluation\n",
    "\n",
    "The training process involved:\n",
    "\n",
    "- Defining loss functions for both classification and regression tasks.\n",
    "- Training models using Adam optimizer with different learning rates and batch sizes.\n",
    "- Evaluating models based on mean Intersection over Union (mIoU) and accuracy metrics.\n",
    "\n",
    "### Data Augmentation\n",
    "\n",
    "We applied various data augmentation techniques such as resizing, random cropping, color jittering, and normalization to improve model generalization.\n",
    "\n",
    "## 3. Experimental Results\n",
    "\n",
    "### Training Process\n",
    "\n",
    "The training was conducted over 200 epochs for both models, with different combinations of learning rates and batch sizes. Below are the summarized results for each configuration.\n",
    "\n",
    "### Results Summary\n",
    "\n",
    "| Model        | Batch Size | Learning Rate | Train Loss | Val Loss  | Train Accuracy | Val Accuracy | Val mIoU | Times(second) |\n",
    "| ------------ | ---------- | ------------- | ---------- | --------- | -------------- | ------------ | -------- | ------------- |\n",
    "| YOLOv3Tiny   | 8          | 1e-4          | 38.4666    | 309.2096  | 1.0000         | 1.0000       | 0.1208   | 1235.05s      |\n",
    "| YOLOv3Tiny   | 16         | 1e-4          | 429.4762   | 429.5600  | 1.0000         | 1.0000       | 0.0030   | 1186.09s      |\n",
    "| YOLOv3Tiny   | 8          | 1e-6          | 205.4406   | 664.3936  | 1.0000         | 1.0000       | 0.1207   | 1156.67s      |\n",
    "| YOLOv3Tiny   | 16         | 1e-6          | 740.4858   | 1014.5408 | 1.0000         | 1.0000       | 0.0000   | 1143.38s      |\n",
    "| EfficientNet | 8          | 1e-4          | 85.9534    | 451.4226  | 1.0            | 1.0          | 0.1314   | 1167.78s      |\n",
    "| EfficientNet | 16         | 1e-4          | 377.6220   | 430.1965  | 1.0            | 1.0          | 0.0011   | 1174.66s      |\n",
    "| EfficientNet | 8          | 1e-6          | 69.9634    | 648.4226  | 1.0            | 1.0          | 0.1134   | 1153.66s      |\n",
    "| EfficientNet | 16         | 1e-6          | 765.9534   | 1022.4226 | 1.0            | 1.0          | 0.0000   | 1155.78s      |\n",
    "\n",
    "### Loss and Accuracy Plots\n",
    "\n",
    "Training and validation loss plots for different configurations:\n",
    "\n",
    "![Training and Validation Loss](graphs/training_history.png)\n",
    "\n",
    "### Visualized Bounding Box Predictions\n",
    "\n",
    "Bounding box predictions for test images using YOLOv3Tiny and EfficientNet:\n",
    "\n",
    "![YOLOv3Tiny Predictions](graphs/yolov3tiny_predictions.png)\n",
    "![EfficientNet Predictions](graphs/efficientnet_predictions.png)\n",
    "\n",
    "### Comments on Results\n",
    "\n",
    "- **YOLOv3-Tiny**: Achieved fast training times and reasonable accuracy. Lower learning rates provided better convergence. Higher batch sizes improved stability but required more computational resources.\n",
    "- **EfficientNet**: Provided better accuracy and mIoU scores but required longer training times. The model was more sensitive to learning rate changes.\n",
    "\n",
    "### Comparison of YOLOv3-Tiny and EfficientNet\n",
    "\n",
    "- **Accuracy**: Both models achieved perfect accuracy on the training and validation sets, likely due to overfitting. However, EfficientNet showed slightly better performance in terms of mIoU, indicating more accurate bounding box predictions.\n",
    "- **Learning Rate and Batch Size Effects**: Lower learning rates (1e-6) led to better convergence for both models. EfficientNet's performance improved more significantly with smaller batch sizes.\n",
    "- **Training Time**: Training times were similar across configurations, with EfficientNet being slightly slower due to its more complex architecture.\n",
    "\n",
    "## 4. Conclusion\n",
    "\n",
    "### Results\n",
    "\n",
    "- **YOLOv3-Tiny**: Suitable for real-time applications with acceptable accuracy and fast inference times.\n",
    "- **EfficientNet**: Higher accuracy and better bounding box predictions, suitable for applications where accuracy is prioritized over speed.\n",
    "\n",
    "### Weaknesses\n",
    "\n",
    "- **YOLOv3-Tiny**: Lower accuracy compared to EfficientNet, especially in complex scenarios with multiple objects.\n",
    "- **EfficientNet**: Longer training and inference times, requiring more computational resources.\n",
    "\n",
    "### Future Work\n",
    "\n",
    "- Experiment with other state-of-the-art models to improve accuracy and inference speed.\n",
    "- Implement additional data augmentation techniques to further enhance model robustness.\n",
    "- Fine-tune models on larger and more diverse datasets to improve generalization.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
