{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lvZGRKnvWIPV"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from torchsummary import summary\n",
    "from torch.utils.data import Subset, DataLoader\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    ConfusionMatrixDisplay,\n",
    "    classification_report,\n",
    ")\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ALEoAmgX6Imz",
    "outputId": "883500cd-3bfd-48cc-b8b8-8aae0b6b5ff6"
   },
   "outputs": [],
   "source": [
    "# Check for GPU availability and set the appropriate device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using {device.type.upper()} for computation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants\n",
    "BATCH_SIZE = 64\n",
    "IMAGE_SIZE = (128, 128)\n",
    "DATASET_PATH = r\"C:\\Users\\aliseydi\\Git\\Assignment 3\\dataset\\raw-img\"\n",
    "\n",
    "# create a folder named graphs to save the model summary\n",
    "if not os.path.exists(\"graphs\"):\n",
    "    os.makedirs(\"graphs\")\n",
    "\n",
    "# create a folder named models\n",
    "if not os.path.exists(\"models\"):\n",
    "    os.makedirs(\"models\")\n",
    "\n",
    "# create a folder named logs\n",
    "if not os.path.exists(\"logs\"):\n",
    "    os.makedirs(\"logs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZNLSiEvodWRg",
    "outputId": "f6ceb6f6-a5ed-4892-fb1d-1195cf6b8a7b"
   },
   "outputs": [],
   "source": [
    "print(f\"Number of classes: {len(os.listdir(DATASET_PATH))}\")\n",
    "\n",
    "# Print number of images per class\n",
    "for folder_name in os.listdir(DATASET_PATH):\n",
    "    path = os.path.join(DATASET_PATH, folder_name)\n",
    "    print(f\"{folder_name}: {len(os.listdir(path))} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformations for training\n",
    "data_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(IMAGE_SIZE),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "dataset = datasets.ImageFolder(DATASET_PATH, transform=data_transforms)\n",
    "NUM_CLASSES = len(dataset.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3_Bm5ZN3YRCz"
   },
   "outputs": [],
   "source": [
    "# Splitting dataset into train, validation, and test sets\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = int(0.1 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(\n",
    "    dataset, [train_size, val_size, test_size]\n",
    ")\n",
    "\n",
    "# Sample a subset for each dataset to speed up training during development\n",
    "train_subset = Subset(train_dataset, range(500))\n",
    "val_subset = Subset(val_dataset, range(100))\n",
    "test_subset = Subset(test_dataset, range(100))\n",
    "\n",
    "\n",
    "# Define data loaders with a smaller batch size for actual training and evaluation\n",
    "train_dataloader = DataLoader(\n",
    "    train_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2\n",
    ")\n",
    "val_dataloader = DataLoader(\n",
    "    val_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    test_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_examples(loader):\n",
    "    \"\"\"Find one example per class from the dataset\"\"\"\n",
    "    examples_per_class = {}\n",
    "    for images, labels in loader:\n",
    "        for i, label in enumerate(labels):\n",
    "            label = label.item()\n",
    "            if label not in examples_per_class:\n",
    "                examples_per_class[label] = images[i]  # store the first occurrence\n",
    "            if len(examples_per_class) == NUM_CLASSES:\n",
    "                break\n",
    "        if len(examples_per_class) == NUM_CLASSES:\n",
    "            break\n",
    "    return examples_per_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_images(examples, title):\n",
    "    \"\"\"Show images with labels, auto-scaling images based on data type.\"\"\"\n",
    "    plt.figure(figsize=(15, 10))\n",
    "\n",
    "    # sort exapmles by class index\n",
    "    examples = dict(sorted(examples.items()))\n",
    "\n",
    "    for idx, (label, image) in enumerate(examples.items()):\n",
    "        ax = plt.subplot(\n",
    "            2, 5, idx + 1\n",
    "        )  # Adjust subplot parameters for number of classes\n",
    "        ax.axis(\"off\")\n",
    "        ax.set_title(dataset.classes[label])\n",
    "\n",
    "        # Convert PyTorch tensor to numpy array after adjusting the channel dimension\n",
    "        if image.dtype == torch.float32:\n",
    "            img = image.numpy()\n",
    "            if img.min() < 0 or img.max() > 1:\n",
    "                img = (img - img.min()) / (img.max() - img.min())  # Normalize to [0, 1]\n",
    "            img = img.transpose(1, 2, 0)  # Correct ordering for matplotlib\n",
    "        elif image.dtype == torch.uint8:\n",
    "            img = image.numpy() / 255.0  # Scale to [0, 1]\n",
    "            img = img.transpose(1, 2, 0)  # Correct ordering for matplotlib\n",
    "        else:\n",
    "            img = image.numpy()\n",
    "            img = img.transpose(1, 2, 0)  # Correct ordering for matplotlib\n",
    "\n",
    "        plt.imshow(img)  # Show the image\n",
    "    plt.suptitle(title, fontsize=16, weight=\"bold\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # save the plot to graphs folsder using title as the name\n",
    "    plt.savefig(f\"graphs/{title}.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find and display examples from each dataset\n",
    "train_examples = find_examples(train_dataloader)\n",
    "show_images(train_examples, \"Examples from Each Class - Training Set\")\n",
    "\n",
    "val_examples = find_examples(val_dataloader)\n",
    "show_images(val_examples, \"Examples from Each Class - Validation Set\")\n",
    "\n",
    "test_examples = find_examples(test_dataloader)\n",
    "show_images(test_examples, \"Examples from Each Class - Testing Set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Without Dropout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E0YjF81Cbkqo"
   },
   "outputs": [],
   "source": [
    "class CustomCNN(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(CustomCNN, self).__init__()\n",
    "        # Initial convolution layer\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)  # Output: 224x224\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)  # Output: 112x112\n",
    "\n",
    "        # Additional convolution layers with reduced number of filters\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)  # Output: 112x112\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)  # Output: 56x56\n",
    "\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)  # Output: 56x56\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)  # Output: 28x28\n",
    "\n",
    "        self.conv4 = nn.Conv2d(128, 128, kernel_size=3, padding=1)  # Output: 28x28\n",
    "        self.relu4 = nn.ReLU()\n",
    "\n",
    "        self.conv5 = nn.Conv2d(128, 64, kernel_size=3, padding=1)  # Output: 28x28\n",
    "        self.relu5 = nn.ReLU()\n",
    "\n",
    "        self.conv6 = nn.Conv2d(64, 32, kernel_size=3, padding=1)  # Output: 28x28\n",
    "        self.relu6 = nn.ReLU()\n",
    "        self.pool6 = nn.MaxPool2d(kernel_size=2, stride=2)  # Output: 14x14\n",
    "\n",
    "        # Global average pooling\n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool2d(\n",
    "            (1, 1)\n",
    "        )  # Reduces each channel to 1x1\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(32, 128)  # Reduced input features after global pooling\n",
    "        self.relu7 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.pool1(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.pool2(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.pool3(x)\n",
    "\n",
    "        x = self.conv4(x)\n",
    "        x = self.relu4(x)\n",
    "\n",
    "        x = self.conv5(x)\n",
    "        x = self.relu5(x)\n",
    "\n",
    "        x = self.conv6(x)\n",
    "        x = self.relu6(x)\n",
    "        x = self.pool6(x)\n",
    "\n",
    "        x = self.global_avg_pool(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten the output\n",
    "\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu7(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Instantiate the model\n",
    "model = CustomCNN(num_classes=10).to(device)\n",
    "\n",
    "# Print the model summary for an input size of (3, 224, 224)\n",
    "summary(model, input_size=(3, IMAGE_SIZE[0], IMAGE_SIZE[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "def train_model(\n",
    "    logger,\n",
    "    num_epochs,\n",
    "    save_path=\"best_model.pth\",\n",
    "    batch_size=64,\n",
    "    learning_rate=0.001,\n",
    "):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Define data loaders with the actual batch size\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        train_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2\n",
    "    )\n",
    "    val_dataloader = DataLoader(\n",
    "        val_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2\n",
    "    )\n",
    "\n",
    "    model.train()\n",
    "    best_f1 = 0.0  # Initialize the best F1 score\n",
    "\n",
    "    # Lists to store metrics for plotting\n",
    "    train_losses, val_accuracies, val_f1s, val_precisions, val_recalls = (\n",
    "        [],\n",
    "        [],\n",
    "        [],\n",
    "        [],\n",
    "        [],\n",
    "    )\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for images, labels in train_dataloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "\n",
    "        # Calculate training epoch loss\n",
    "        epoch_loss = running_loss / len(train_dataset)\n",
    "        train_losses.append(epoch_loss)\n",
    "\n",
    "        logger.info(f\"Epoch [{epoch+1}/{num_epochs}], Training Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_labels = []\n",
    "        val_preds = []\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_dataloader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                val_labels.extend(labels.cpu().numpy())\n",
    "                val_preds.extend(predicted.cpu().numpy())\n",
    "\n",
    "        # Calculate metrics\n",
    "        accuracy = accuracy_score(val_labels, val_preds)\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "            val_labels, val_preds, average=\"weighted\"\n",
    "        )\n",
    "\n",
    "        val_accuracies.append(accuracy)\n",
    "        val_f1s.append(f1)\n",
    "        val_precisions.append(precision)\n",
    "        val_recalls.append(recall)\n",
    "\n",
    "        logger.info(\n",
    "            f\"Validation Metrics - Acc: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}\"\n",
    "        )\n",
    "\n",
    "        # Save the model if it has the best F1 score so far\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            torch.save(model.state_dict(), f\"models/{save_path}\")\n",
    "            logger.info(f\"Model saved: Improved F1 from {best_f1:.4f} to {f1:.4f}\")\n",
    "\n",
    "    # close logger\n",
    "    logger.handlers.clear()\n",
    "\n",
    "    # remove logger object\n",
    "    del logger\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(train_losses, label=\"Training Loss\")\n",
    "    plt.plot(val_accuracies, label=\"Validation Accuracy\")\n",
    "\n",
    "    # use batch size and learning rate as the name of the plot\n",
    "    plt.title(\n",
    "        f\"Training Loss, Validation Accuracy, and F1 Score - {batch_size}_{learning_rate}\"\n",
    "    )\n",
    "\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Metrics\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(val_precisions, label=\"Validation Precision\")\n",
    "    plt.plot(val_recalls, label=\"Validation Recall\")\n",
    "    plt.plot(val_f1s, label=\"Validation F1 Score\")\n",
    "\n",
    "    # use batch size and learning rate as the name of the plot\n",
    "    plt.title(f\"Validation Precision and Recall - {batch_size}_{learning_rate}\")\n",
    "\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Metrics\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # save graphs to the graphs folder using batch size and learning rate model name as the nname\n",
    "    plt.savefig(f\"graphs/{batch_size}_{learning_rate}_{save_path}.png\")\n",
    "\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model_path, test_dataloader):\n",
    "    model.load_state_dict(torch.load(f\"models/{model_path}\"))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_dataloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "    # Display classification report\n",
    "    print(\"Classification Report:\")\n",
    "    print(\n",
    "        classification_report(\n",
    "            all_labels,\n",
    "            all_preds,\n",
    "            target_names=[f\"Class {i}\" for i in range(len(np.unique(all_labels)))],\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Calculate the confusion matrix\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    display_labels = [f\"Class {i}\" for i in range(len(cm))]\n",
    "\n",
    "    # Display the confusion matrix\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    ConfusionMatrixDisplay(cm, display_labels=display_labels).plot(\n",
    "        values_format=\"d\", ax=ax\n",
    "    )\n",
    "\n",
    "    # use model path as the title of the plot\n",
    "    plt.title(f\"Confusion Matrix - {model_path}\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # save the confusion matrix to the graphs folder using model name as the name\n",
    "    plt.savefig(f\"graphs/conf_matrix_{model_path}.png\")\n",
    "\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create logger function to save model training logs using python logging which is save to a file\n",
    "import logging\n",
    "\n",
    "\n",
    "def create_logger(filename=\"logs.log\"):\n",
    "    logging.basicConfig(\n",
    "        filename=f\"logs/{filename}\",\n",
    "        level=logging.INFO,\n",
    "        format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n",
    "    )\n",
    "\n",
    "    # create a logger\n",
    "    logger = logging.getLogger(__name__)\n",
    "    return logger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rates = [0.001, 0.0001, 0.000001]\n",
    "batch_sizes = [32, 64]\n",
    "\n",
    "for learnig_rate in learning_rates:\n",
    "    for batch_size in batch_sizes:\n",
    "        model = CustomCNN(num_classes=10).to(device)\n",
    "        logger = create_logger(f\"lr_{learnig_rate}_bs_{batch_size}.log\")\n",
    "        train_model(\n",
    "            num_epochs=100,\n",
    "            save_path=f\"best_model_lr_{learnig_rate}_bs_{batch_size}.pth\",\n",
    "            batch_size=batch_size,\n",
    "            learning_rate=learnig_rate,\n",
    "            logger=logger,\n",
    "        )\n",
    "\n",
    "        evaluate_model(\n",
    "            f\"best_model_lr_{learnig_rate}_bs_{batch_size}.pth\", test_dataloader\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With Dropout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDropCNN(nn.Module):\n",
    "    def __init__(self, num_classes=10, dropout_rate=0.5):\n",
    "        super(CustomDropCNN, self).__init__()\n",
    "        # Initial convolution layer\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)  # Output: 224x224\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)  # Output: 112x112\n",
    "\n",
    "        # Additional convolution layers with reduced number of filters\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)  # Output: 112x112\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)  # Output: 56x56\n",
    "\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)  # Output: 56x56\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)  # Output: 28x28\n",
    "\n",
    "        self.conv4 = nn.Conv2d(128, 128, kernel_size=3, padding=1)  # Output: 28x28\n",
    "        self.relu4 = nn.ReLU()\n",
    "\n",
    "        self.conv5 = nn.Conv2d(128, 64, kernel_size=3, padding=1)  # Output: 28x28\n",
    "        self.relu5 = nn.ReLU()\n",
    "\n",
    "        self.conv6 = nn.Conv2d(64, 32, kernel_size=3, padding=1)  # Output: 28x28\n",
    "        self.relu6 = nn.ReLU()\n",
    "        self.pool6 = nn.MaxPool2d(kernel_size=2, stride=2)  # Output: 14x14\n",
    "\n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "        # Global average pooling\n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool2d(\n",
    "            (1, 1)\n",
    "        )  # Reduces each channel to 1x1\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(32, 128)  # Reduced input features after global pooling\n",
    "        self.relu7 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.pool1(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.pool2(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.pool3(x)\n",
    "\n",
    "        x = self.conv4(x)\n",
    "        x = self.relu4(x)\n",
    "\n",
    "        x = self.conv5(x)\n",
    "        x = self.relu5(x)\n",
    "\n",
    "        x = self.conv6(x)\n",
    "        x = self.relu6(x)\n",
    "        x = self.pool6(x)\n",
    "\n",
    "        x = self.global_avg_pool(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten the output\n",
    "\n",
    "        x = self.dropout(x)  # Apply dropout before entering the fully connected layer\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu7(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "model = CustomDropCNN(num_classes=10).to(device)\n",
    "\n",
    "summary(model, input_size=(3, IMAGE_SIZE[0], IMAGE_SIZE[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout_rates = [0.25, 0.5, 0.75, 1.0]\n",
    "\n",
    "for rate in dropout_rates:\n",
    "    model = CustomDropCNN(num_classes=10, dropout_rate=rate).to(device)\n",
    "    logger = create_logger(f\"dropout_{rate}.log\")\n",
    "    train_model(\n",
    "        logger=logger,\n",
    "        num_epochs=100,\n",
    "        save_path=f\"best_model_dropout_{rate}.pth\",\n",
    "        batch_size=64,\n",
    "        learning_rate=0.001,\n",
    "    )\n",
    "\n",
    "    evaluate_model(f\"best_model_dropout_{rate}.pth\", test_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train FC Layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "\n",
    "\n",
    "class EfficientNetB0Custom(nn.Module):\n",
    "    def __init__(self, num_classes=10, dropout_rate=0.5):\n",
    "        super(EfficientNetB0Custom, self).__init__()\n",
    "        # Load the pre-trained EfficientNet-B0 model\n",
    "        self.efficientnet_b0 = models.efficientnet_b0(pretrained=True)\n",
    "\n",
    "        # Replace the classifier part of the EfficientNet-B0\n",
    "        self.efficientnet_b0.classifier = nn.Sequential(\n",
    "            nn.Linear(\n",
    "                self.efficientnet_b0.classifier[1].in_features, 128\n",
    "            ),  # Adapted to the new structure\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Use the modified EfficientNet-B0 to perform the forward pass\n",
    "        x = self.efficientnet_b0(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "model = EfficientNetB0Custom(num_classes=10).to(device)\n",
    "\n",
    "summary(model, input_size=(3, IMAGE_SIZE[0], IMAGE_SIZE[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EfficientNetB0Custom(num_classes=10).to(device)\n",
    "train_model(\n",
    "    logger=logger,\n",
    "    num_epochs=100,\n",
    "    save_path=\"best_model_efficient_64_1e-4.pth\",\n",
    "    batch_size=64,\n",
    "    learning_rate=0.001,\n",
    ")\n",
    "\n",
    "evaluate_model(\"best_model_efficient_64_1e-4.pth\", test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EfficientNetB0CustomConv(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(EfficientNetB0CustomConv, self).__init__()\n",
    "        self.base_model = models.efficientnet_b0(pretrained=True)\n",
    "        self.setup_base_model()\n",
    "        self.redefine_classifier(num_classes)\n",
    "\n",
    "    def setup_base_model(self):\n",
    "        # Freeze all parameters initially\n",
    "        for param in self.base_model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Unfreeze the parameters of the last two convolutional blocks\n",
    "        for param in self.base_model.features[-2:].parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    def redefine_classifier(self, num_classes):\n",
    "        # Redefine the classifier to fit the number of classes\n",
    "        in_features = self.base_model.classifier[1].in_features\n",
    "        self.base_model.classifier = nn.Sequential(\n",
    "            nn.Linear(in_features, 128),  # First fully connected layer\n",
    "            nn.ReLU(),  # Activation layer\n",
    "            nn.Linear(128, num_classes),  # Final layer for class prediction\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass through the base model\n",
    "        return self.base_model(x)\n",
    "\n",
    "\n",
    "model = EfficientNetB0CustomConv(num_classes=10).to(device)\n",
    "\n",
    "# Display the model summary\n",
    "summary(model, input_size=(3, IMAGE_SIZE[0], IMAGE_SIZE[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EfficientNetB0Custom(num_classes=10).to(device)\n",
    "train_model(\n",
    "    logger=logger,\n",
    "    num_epochs=100,\n",
    "    save_path=\"best_model_efficient_64_1e-4_conv.pth\",\n",
    "    batch_size=64,\n",
    "    learning_rate=0.001,\n",
    ")\n",
    "\n",
    "evaluate_model(\"best_model_efficient_64_1e-4_conv.pth\", test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "gpuType": "V28",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
