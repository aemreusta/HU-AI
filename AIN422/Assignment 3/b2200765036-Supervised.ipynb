{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ahmet Emre Usta\n",
    "\n",
    "## 2200765036\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score, f1_score, matthews_corrcoef\n",
    "from sklearn.model_selection import KFold\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data\n",
    "\n",
    "load_rna_data(file_path): Reads the RNA sequences from a file, processes each entry, and stores them in a dictionary.\n",
    "\n",
    "load_hgnc_data(file_path): Reads HGNC data from a CSV file into a DataFrame, ensuring correct data types.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load RNA sequences\n",
    "def load_rna_data(file_path):\n",
    "    with open(file_path, \"r\") as file:\n",
    "        data = file.read().split(\">\")\n",
    "    rna_sequences = {}\n",
    "    for entry in data[1:]:\n",
    "        lines = entry.strip().split(\"\\n\")\n",
    "        header = lines[0].split()[0].split(\".\")[0]\n",
    "        sequence = \"\".join(lines[1:])\n",
    "        rna_sequences[header] = sequence\n",
    "    print(f\"Loaded {len(rna_sequences)} RNA sequences\")\n",
    "    return rna_sequences\n",
    "\n",
    "\n",
    "# Load HGNC data\n",
    "def load_hgnc_data(file_path):\n",
    "    dtype = {\n",
    "        \"HGNC ID\": str,\n",
    "        \"Approved symbol\": str,\n",
    "        \"Approved name\": str,\n",
    "        \"Chromosome location\": str,\n",
    "        \"Chromosome\": str,\n",
    "        \"Locus group\": str,\n",
    "        \"Locus type\": str,\n",
    "        \"HGNC family ID\": str,\n",
    "        \"HGNC family name\": str,\n",
    "        \"RefSeq accession\": str,\n",
    "        \"NCBI gene ID\": str,\n",
    "        \"Ensembl gene ID\": str,\n",
    "    }\n",
    "    df = pd.read_csv(file_path, delimiter=\"\\t\", dtype=dtype)\n",
    "    print(\"Loaded HGNC data with shape:\", df.shape)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging Data\n",
    "\n",
    "merge_data(df, rna_sequences): Merges the RNA sequences with the HGNC data based on the RefSeq accession, drops any rows with missing sequences, and returns the merged DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge RNA sequences with HGNC data\n",
    "def merge_data(df, rna_sequences):\n",
    "    df[\"Sequence\"] = df[\"RefSeq accession\"].map(rna_sequences)\n",
    "    missing_sequences = df[\"Sequence\"].isnull().sum()\n",
    "    df = df.dropna(subset=[\"Sequence\"]).copy()\n",
    "    print(f\"Merged data contains {df.shape[0]} sequences\")\n",
    "    print(f\"Missing sequences: {missing_sequences}\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning and Encoding Sequences\n",
    "\n",
    "clean_gene_string(sequence): Cleans each RNA sequence by replacing invalid characters with 'X'.\n",
    "\n",
    "one_hot_encode_sequence(sequence): One-hot encodes the cleaned RNA sequences, mapping each nucleotide to a binary vector.\n",
    "\n",
    "pad_sequences(sequences, maxlen): Pads or truncates sequences to a fixed length (maxlen), ensuring uniform input size for the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean RNA sequences\n",
    "def clean_gene_string(sequence):\n",
    "    valid_chars = {\"A\", \"T\", \"C\", \"G\"}\n",
    "    return \"\".join([char if char in valid_chars else \"X\" for char in sequence])\n",
    "\n",
    "\n",
    "# One-hot encode RNA sequences\n",
    "def one_hot_encode_sequence(sequence):\n",
    "    mapping = {\n",
    "        \"A\": [1, 0, 0, 0, 0],\n",
    "        \"T\": [0, 1, 0, 0, 0],\n",
    "        \"C\": [0, 0, 1, 0, 0],\n",
    "        \"G\": [0, 0, 0, 1, 0],\n",
    "        \"X\": [0, 0, 0, 0, 1],\n",
    "    }\n",
    "    return [mapping[char] for char in sequence]\n",
    "\n",
    "\n",
    "# Pad sequences to the same length\n",
    "def pad_sequences(sequences, maxlen=1000):\n",
    "    padded_sequences = np.zeros((len(sequences), maxlen, 5), dtype=int)\n",
    "    for i, seq in enumerate(sequences):\n",
    "        length = min(len(seq), maxlen)\n",
    "        padded_sequences[i, :length, :] = seq[:length]\n",
    "    return padded_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Data:\n",
    "\n",
    "preprocess_data(hgnc_file_path, rna_file_path, maxlen): Orchestrates the loading, merging, cleaning, encoding, and padding steps to prepare the data for model training. Limits the number of samples to avoid memory issues.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess data\n",
    "def preprocess_data(hgnc_file_path, rna_file_path, maxlen=1000):\n",
    "    # Load data\n",
    "    hgnc_data = load_hgnc_data(hgnc_file_path)\n",
    "    rna_sequences = load_rna_data(rna_file_path)\n",
    "\n",
    "    # Merge data\n",
    "    merged_data = merge_data(hgnc_data, rna_sequences)\n",
    "\n",
    "    # Clean sequences\n",
    "    merged_data[\"Sequence\"] = merged_data[\"Sequence\"].apply(clean_gene_string)\n",
    "\n",
    "    # Limit the size to avoid memory issues\n",
    "    merged_data = merged_data.iloc[:20000]\n",
    "\n",
    "    # One-hot encode sequences\n",
    "    one_hot_encoded_sequences = [\n",
    "        one_hot_encode_sequence(seq) for seq in merged_data[\"Sequence\"]\n",
    "    ]\n",
    "\n",
    "    # Pad sequences\n",
    "    X = pad_sequences(one_hot_encoded_sequences, maxlen)\n",
    "\n",
    "    # Encode labels\n",
    "    y = pd.get_dummies(merged_data[\"Locus group\"]).values\n",
    "\n",
    "    print(\n",
    "        f\"Preprocessed data into {X.shape[0]} samples with {X.shape[1]} time steps each\"\n",
    "    )\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics Calculation:\n",
    "\n",
    "Functions to calculate accuracy (calculate_accuracy), F1 score (calculate_f1), and Matthews correlation coefficient (calculate_mcc) for model evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to calculate accuracy\n",
    "def calculate_accuracy(y_true, y_pred):\n",
    "    y_true = y_true.argmax(dim=1)\n",
    "    y_pred = y_pred.argmax(dim=1)\n",
    "    return accuracy_score(y_true.cpu(), y_pred.cpu())\n",
    "\n",
    "\n",
    "# Define function to calculate F1 score\n",
    "def calculate_f1(y_true, y_pred):\n",
    "    y_true = y_true.argmax(dim=1)\n",
    "    y_pred = y_pred.argmax(dim=1)\n",
    "    return f1_score(y_true.cpu(), y_pred.cpu(), average=\"weighted\")\n",
    "\n",
    "\n",
    "# Define function to calculate MCC\n",
    "def calculate_mcc(y_true, y_pred):\n",
    "    y_true = y_true.argmax(dim=1)\n",
    "    y_pred = y_pred.argmax(dim=1)\n",
    "    return matthews_corrcoef(y_true.cpu(), y_pred.cpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN Model Definition:\n",
    "\n",
    "RNNModel(nn.Module): Defines a simple RNN model with one RNN layer followed by a fully connected layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the RNN model in PyTorch\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.rnn(x)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "\n",
    "# Adjust RNADataset to avoid out-of-bounds errors\n",
    "class RNADataset(Dataset):\n",
    "    def __init__(self, sequences, labels):\n",
    "        assert len(sequences) == len(labels), \"Mismatch between sequences and labels\"\n",
    "        self.sequences = sequences\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.sequences[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training with Cross-Validation:\n",
    "\n",
    "train_model_cv(X, y, k_folds): Trains the RNN model using k-fold cross-validation. Tracks training and validation metrics, saves the best model based on validation F1 score, and plots training history.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model with cross-validation\n",
    "def train_model_cv(X, y, k_folds=5):\n",
    "    kfold = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "    fold_results = []\n",
    "\n",
    "    input_size = X.shape[2]  # One-hot encoding size\n",
    "    hidden_size = 50\n",
    "    output_size = y.shape[1]  # Number of classes\n",
    "    num_epochs = 10\n",
    "    learning_rate = 0.001\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Device: {device}\")\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(kfold.split(X, y)):\n",
    "        print(f\"Fold {fold + 1}\")\n",
    "\n",
    "        model = RNNModel(input_size, hidden_size, output_size).to(device)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "        if fold == 0:\n",
    "            print(summary(model, (X.shape[1], X.shape[2])))\n",
    "\n",
    "        print(\n",
    "            f\"Train indices length: {len(train_idx)}, Validation indices length: {len(val_idx)}\"\n",
    "        )\n",
    "        print(\n",
    "            f\"Max train index: {max(train_idx)}, Max validation index: {max(val_idx)}\"\n",
    "        )\n",
    "\n",
    "        train_subsampler = torch.utils.data.SubsetRandomSampler(train_idx)\n",
    "        val_subsampler = torch.utils.data.SubsetRandomSampler(val_idx)\n",
    "\n",
    "        train_loader = DataLoader(\n",
    "            RNADataset(X, y),  # Pass the full dataset and use subsampler for indices\n",
    "            batch_size=32,\n",
    "            sampler=train_subsampler,\n",
    "        )\n",
    "        val_loader = DataLoader(\n",
    "            RNADataset(X, y),  # Pass the full dataset and use subsampler for indices\n",
    "            batch_size=32,\n",
    "            sampler=val_subsampler,\n",
    "        )\n",
    "\n",
    "        fold_history = {\n",
    "            \"fold\": fold + 1,\n",
    "            \"train_time\": None,\n",
    "            \"best_epoch\": None,\n",
    "            \"train_loss\": None,\n",
    "            \"train_acc\": None,\n",
    "            \"train_f1\": None,\n",
    "            \"train_mcc\": None,\n",
    "            \"val_loss\": None,\n",
    "            \"val_acc\": None,\n",
    "            \"val_f1\": None,\n",
    "            \"val_mcc\": None,\n",
    "        }\n",
    "\n",
    "        history = {\n",
    "            \"fold\": fold + 1,\n",
    "            \"train_loss\": [],\n",
    "            \"train_acc\": [],\n",
    "            \"train_f1\": [],\n",
    "            \"train_mcc\": [],\n",
    "            \"val_loss\": [],\n",
    "            \"val_acc\": [],\n",
    "            \"val_f1\": [],\n",
    "            \"val_mcc\": [],\n",
    "        }\n",
    "\n",
    "        start_time = time.time()\n",
    "        best_epoch_val_f1 = 0\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            train_losses = []\n",
    "            train_accuracies = []\n",
    "            train_f1_scores = []\n",
    "            train_mcc_scores = []\n",
    "\n",
    "            for sequences, labels in train_loader:\n",
    "                sequences = sequences.float().to(device)\n",
    "                labels = labels.float().to(device)\n",
    "\n",
    "                outputs = model(sequences)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                train_losses.append(loss.item())\n",
    "                train_acc = calculate_accuracy(labels, outputs)\n",
    "                train_f1 = calculate_f1(labels, outputs)\n",
    "                train_mcc = calculate_mcc(labels, outputs)\n",
    "                train_accuracies.append(train_acc)\n",
    "                train_f1_scores.append(train_f1)\n",
    "                train_mcc_scores.append(train_mcc)\n",
    "\n",
    "            avg_train_loss = np.mean(train_losses)\n",
    "            avg_train_acc = np.mean(train_accuracies)\n",
    "            avg_train_f1 = np.mean(train_f1_scores)\n",
    "            avg_train_mcc = np.mean(train_mcc_scores)\n",
    "\n",
    "            model.eval()\n",
    "            val_losses = []\n",
    "            val_accuracies = []\n",
    "            val_f1_scores = []\n",
    "            val_mcc_scores = []\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for sequences, labels in val_loader:\n",
    "                    sequences = sequences.float().to(device)\n",
    "                    labels = labels.float().to(device)\n",
    "\n",
    "                    outputs = model(sequences)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    val_losses.append(loss.item())\n",
    "                    val_acc = calculate_accuracy(labels, outputs)\n",
    "                    val_f1 = calculate_f1(labels, outputs)\n",
    "                    val_mcc = calculate_mcc(labels, outputs)\n",
    "                    val_accuracies.append(val_acc)\n",
    "                    val_f1_scores.append(val_f1)\n",
    "                    val_mcc_scores.append(val_mcc)\n",
    "\n",
    "            avg_val_loss = np.mean(val_losses)\n",
    "            avg_val_acc = np.mean(val_accuracies)\n",
    "            avg_val_f1 = np.mean(val_f1_scores)\n",
    "            avg_val_mcc = np.mean(val_mcc_scores)\n",
    "\n",
    "            history[\"train_loss\"].append(avg_train_loss)\n",
    "            history[\"train_acc\"].append(avg_train_acc)\n",
    "            history[\"train_f1\"].append(avg_train_f1)\n",
    "            history[\"train_mcc\"].append(avg_train_mcc)\n",
    "            history[\"val_loss\"].append(avg_val_loss)\n",
    "            history[\"val_acc\"].append(avg_val_acc)\n",
    "            history[\"val_f1\"].append(avg_val_f1)\n",
    "            history[\"val_mcc\"].append(avg_val_mcc)\n",
    "\n",
    "            print(\n",
    "                f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, Train Acc: {avg_train_acc:.4f}, Train F1: {avg_train_f1:.4f}, Train MCC: {avg_train_mcc:.4f}, Val Loss: {avg_val_loss:.4f}, Val Acc: {avg_val_acc:.4f}, Val F1: {avg_val_f1:.4f}, Val MCC: {avg_val_mcc:.4f}\"\n",
    "            )\n",
    "\n",
    "            if avg_val_f1 > best_epoch_val_f1:\n",
    "                best_epoch_val_f1 = avg_val_f1\n",
    "                fold_history[\"best_epoch\"] = epoch + 1\n",
    "                fold_history[\"train_loss\"] = avg_train_loss\n",
    "                fold_history[\"train_acc\"] = avg_train_acc\n",
    "                fold_history[\"train_f1\"] = avg_train_f1\n",
    "                fold_history[\"train_mcc\"] = avg_train_mcc\n",
    "                fold_history[\"val_loss\"] = avg_val_loss\n",
    "                fold_history[\"val_acc\"] = avg_val_acc\n",
    "                fold_history[\"val_f1\"] = avg_val_f1\n",
    "                fold_history[\"val_mcc\"] = avg_val_mcc\n",
    "\n",
    "                torch.save(model.state_dict(), f\"model_fold_{fold + 1}.pt\")\n",
    "\n",
    "        end_time = time.time()\n",
    "        hours, rem = divmod(end_time - start_time, 3600)\n",
    "        minutes, seconds = divmod(rem, 60)\n",
    "        fold_history[\"train_time\"] = \"{:0>2}:{:0>2}:{:05.2f}\".format(\n",
    "            int(hours), int(minutes), seconds\n",
    "        )\n",
    "\n",
    "        print(f\"Fold {fold + 1} training completed in {fold_history['train_time']}\")\n",
    "        plot_training_history(history)\n",
    "        fold_results.append(fold_history)\n",
    "\n",
    "    return fold_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization Functions:\n",
    "\n",
    "plot_training_history(history): Plots training and validation loss, accuracy, F1 score, and MCC for each epoch.\n",
    "\n",
    "visualize_data(df): Visualizes the distribution of locus groups and sequence lengths.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization function\n",
    "def plot_training_history(history):\n",
    "    epochs = range(1, len(history[\"train_loss\"]) + 1)\n",
    "\n",
    "    # Plot training & validation loss values\n",
    "    plt.figure(figsize=(20, 10))\n",
    "\n",
    "    # Subplot for training & validation loss values\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(epochs, history[\"train_loss\"], label=\"Train Loss\")\n",
    "    plt.plot(epochs, history[\"val_loss\"], label=\"Validation Loss\")\n",
    "    plt.title(\"Training and Validation Loss\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "\n",
    "    # Subplot for training & validation accuracy values\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.plot(epochs, history[\"train_acc\"], label=\"Train Accuracy\")\n",
    "    plt.plot(epochs, history[\"val_acc\"], label=\"Validation Accuracy\")\n",
    "    plt.title(\"Training and Validation Accuracy\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend()\n",
    "\n",
    "    # Subplot for training & validation F1-Score values\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.plot(epochs, history[\"train_f1\"], label=\"Train F1-Score\")\n",
    "    plt.plot(epochs, history[\"val_f1\"], label=\"Validation F1-Score\")\n",
    "    plt.title(\"Training and Validation F1-Score\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"F1-Score\")\n",
    "    plt.legend()\n",
    "\n",
    "    # Subplot for training & validation MCC values\n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.plot(epochs, history[\"train_mcc\"], label=\"Train MCC\")\n",
    "    plt.plot(epochs, history[\"val_mcc\"], label=\"Validation MCC\")\n",
    "    plt.title(\"Training and Validation MCC\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"MCC\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"training_history_fold_{history['fold']}.png\")\n",
    "\n",
    "    # Visualize the dataset\n",
    "\n",
    "\n",
    "def visualize_data(df):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.countplot(\n",
    "        data=df, x=\"Locus group\", order=df[\"Locus group\"].value_counts().index\n",
    "    )\n",
    "    plt.title(\"Distribution of Locus Groups\")\n",
    "    plt.xlabel(\"Locus Group\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.xticks(rotation=45)\n",
    "    # plt.show()\n",
    "    plt.savefig(\"locus_groups.png\")\n",
    "\n",
    "    df[\"Sequence Length\"] = df[\"Sequence\"].apply(len)\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.histplot(df[\"Sequence Length\"], bins=50, kde=True)\n",
    "    plt.title(\"Distribution of Sequence Lengths\")\n",
    "    plt.xlabel(\"Sequence Length\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    # plt.show()\n",
    "    plt.savefig(\"sequence_lengths.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Function:\n",
    "\n",
    "main(): Executes the full workflow: preprocesses the data, trains the model using cross-validation, loads the best model, generates predictions, and saves the predictions to a file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # File paths\n",
    "    hgnc_file_path = \"HGNC_results.txt\"\n",
    "    rna_file_path = \"GRCh38_latest_rna.fna\"\n",
    "\n",
    "    # Preprocess data\n",
    "    X, y = preprocess_data(hgnc_file_path, rna_file_path, maxlen=1000)\n",
    "\n",
    "    print(\"Data preprocessing complete.\")\n",
    "    print(\"Shape of X:\", X.shape)\n",
    "    print(\"Shape of y:\", y.shape)\n",
    "\n",
    "    fold_results = train_model_cv(X, y)\n",
    "\n",
    "    results_df = pd.DataFrame(fold_results)\n",
    "    results_df.to_csv(\"fold_results.csv\", index=False)\n",
    "\n",
    "    # Load the best model based on validation F1 score\n",
    "    model = RNNModel(X.shape[2], 50, y.shape[1])\n",
    "    best_fold = max(fold_results, key=lambda x: x[\"val_f1\"])[\"fold\"]\n",
    "    model.load_state_dict(torch.load(f\"model_fold_{best_fold}.pt\"))\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    # Generate predictions for the dataset\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for seq in X:\n",
    "            seq_tensor = torch.tensor(seq).unsqueeze(0).float()\n",
    "            output = model(seq_tensor)\n",
    "            pred = output.argmax(dim=1).item()\n",
    "            predictions.append(pred)\n",
    "\n",
    "    # Create a DataFrame for the predictions\n",
    "    hgnc_data = load_hgnc_data(hgnc_file_path)\n",
    "    hgnc_data = hgnc_data.iloc[\n",
    "        : len(predictions)\n",
    "    ]  # Ensure it matches the length of predictions\n",
    "\n",
    "    column_names = pd.get_dummies(\n",
    "        hgnc_data[\"Locus group\"]\n",
    "    ).columns  # Get original column names\n",
    "    hgnc_data[\"Prediction\"] = [column_names[p] for p in predictions]\n",
    "\n",
    "    # Save predictions to a file\n",
    "    hgnc_data[[\"HGNC ID\", \"Prediction\"]].to_csv(\n",
    "        \"HGNC_outputs.txt\", index=False, header=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Issues and Solutions\n",
    "\n",
    "### Memory Issues:\n",
    "\n",
    "Limiting the dataset to 20,000 samples (merged_data = merged_data.iloc[:20000]) helps prevent memory crashes. However, this may lead to loss of potentially useful data. Consider using larger compute resources or more efficient data handling techniques if possible.\n",
    "\n",
    "### Mismatched Sequence Lengths:\n",
    "\n",
    "Ensuring all sequences are of uniform length through padding (pad_sequences()) is crucial for consistent model input.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This code preprocessing of RNA and HGNC data, train an RNN model using cross-validation, and generate predictions. It is crucial to ensure that the data preprocessing steps result in uniform input sizes for the model, and that the cross-validation process effectively tracks and saves the best performing model. Potential issues such as memory constraints and mismatched data lengths should be carefully managed to avoid runtime errors and ensure reliable performance.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
